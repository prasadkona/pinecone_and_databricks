{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dffd49e-6bd0-4a06-a156-6c567bf29710",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 1/ Ingesting and preparing PDF for LLM and Pinecone Vector Database\n",
    "\n",
    "## In this example, we will focus on ingesting pdf documents as source for our retrieval process. \n",
    "\n",
    "<img src=\"https://github.com/prasadkona/databricks_demos/blob/main/images/llm-rag-full-pinecone-1.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "\n",
    "For this example, we will add Databricks ebook PDFs from [Databricks resources page](https://www.databricks.com/resources) to our knowledge database.\n",
    "\n",
    "**Note: This demo is an advanced content, we strongly recommand going over the simple version first to learn the basics.**\n",
    "\n",
    "Here are all the detailed steps:\n",
    "\n",
    "- Use autoloader to load the binary PDF as our first table. \n",
    "- Use the `unstructured` library  to parse the text content of the PDFs.\n",
    "- Use `llama_index` or `Langchain` to split the texts into chuncks.\n",
    "- Compute embeddings for the chunks\n",
    "- Save our text chunks + embeddings in a Delta Lake table\n",
    "- Write to Pinecone vector database.\n",
    "\n",
    "\n",
    "Lakehouse AI not only provides state of the art solutions to accelerate your AI and LLM projects, but also to accelerate data ingestion and preparation at scale, including unstructured data like pdfs.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1444828305810485&notebook=advanced/01-PDF-Advanced-Data-Preparation&demo_name=chatbot-rag-llm&event=VIEW\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8623d962-9c8a-4860-b860-446ade9070ef",
     "showTitle": true,
     "title": "Install required external libraries "
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U transformers==4.41.1 pypdf==4.1.0 langchain-text-splitters==0.2.0 mlflow==2.15.1 tiktoken==0.7.0 torch==2.3.0 llama-index==0.10.43 pinecone-client==5.0.1\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb336533-0732-47cf-960c-3ed84f17a6ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE CATALOG `prasad_kona_dev`\nusing catalog.database `prasad_kona_dev`.`rag_chatbot_prasad_kona`\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../_resources/00-init-advanced $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd2070e-22f6-41c7-a808-f886228aab15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ingesting Databricks ebook PDFs and extracting their pages\n",
    "\n",
    "<img src=\"https://github.com/prasadkona/databricks_demos/blob/main/images/llm-rag-full-pinecone-2.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "First, let's ingest our PDFs as a Delta Lake table with path urls and content in binary format. \n",
    "\n",
    "We'll use [Databricks Autoloader](https://docs.databricks.com/en/ingestion/auto-loader/index.html) to incrementally ingest new files, making it easy to incrementally consume billions of files from the data lake in various data formats. Autoloader can easily ingests our unstructured PDF data in binary format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa35fee2-f035-47a9-864b-7ba80300ce02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS volume_databricks_documentation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76414e4d-1d83-40be-945a-061ca8849f1e",
     "showTitle": true,
     "title": "Our pdf or docx files are available in our Volume (or DBFS)"
    }
   },
   "outputs": [],
   "source": [
    "# List our raw PDF docs\n",
    "volume_folder =  f\"/Volumes/{catalog}/{db}/volume_databricks_documentation\"\n",
    "#Let's upload some pdf to our volume as example\n",
    "upload_pdfs_to_volume(volume_folder+\"/databricks-pdf\")\n",
    "\n",
    "display(dbutils.fs.ls(volume_folder+\"/databricks-pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3769d2e1-2fd9-4450-bd58-499b53390c68",
     "showTitle": true,
     "title": "Ingesting PDF files as binary format using Databricks Autoloader"
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.readStream\n",
    "        .format('cloudFiles')\n",
    "        .option('cloudFiles.format', 'BINARYFILE')\n",
    "        .load('dbfs:'+volume_folder+\"/databricks-pdf\"))\n",
    "\n",
    "# Write the data as a Delta table\n",
    "(df.writeStream\n",
    "  .trigger(availableNow=True)\n",
    "  .option(\"checkpointLocation\", f'dbfs:{volume_folder}/checkpoints/raw_docs')\n",
    "  .table('pdf_raw').awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1168d86d-2335-4e34-8a4e-0f731db1619c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM pdf_raw LIMIT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bb3cf4b-e813-43cf-b38a-37999cac309c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/prasadkona/databricks_demos/blob/main/images/llm-rag-full-pinecone-3.png?raw=true\" style=\"float: right\" width=\"600px\">\n",
    "\n",
    "## Extracting our PDF content as text chunk\n",
    "\n",
    "We need to convert the pdf documents bytes as text, and extract chunks from their content.\n",
    "\n",
    "This part can be tricky as pdf are hard to work with and can be saved as images, for which we'll need an OCR to extract the text.\n",
    "\n",
    "Using the `Unstructured` library within a Spark UDF makes it easy to extract text. \n",
    "\n",
    "*Note: Your cluster will need a few extra libraries that you would typically install with a cluster init script.*\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "\n",
    "### Splitting our big documentation page in smaller chunks\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/chunk-window-size.png?raw=true\" style=\"float: right\" width=\"700px\">\n",
    "\n",
    "In this demo, some PDF can be really big, with a lot of text.\n",
    "\n",
    "We'll extract the content and then use llama_index `SentenceSplitter`, and ensure that each chunk isn't bigger than 500 tokens. \n",
    "\n",
    "\n",
    "The chunk size and chunk overlap depend on the use case and the PDF files. \n",
    "\n",
    "Remember that your prompt+answer should stay below your model max window size (4096 for llama2). \n",
    "\n",
    "For more details, review the previous [../01-Data-Preparation](01-Data-Preparation) notebook. \n",
    "\n",
    "<br/>\n",
    "<br style=\"clear: both\">\n",
    "<div style=\"background-color: #def2ff; padding: 15px;  border-radius: 30px; \">\n",
    "  <strong>Information</strong><br/>\n",
    "  Remember that the following steps are specific to your dataset. This is a critical part to building a successful RAG assistant.\n",
    "  <br/> Always take time to review the chunks created and ensure they make sense, containing relevant informations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceb04867-d34c-41a2-af40-aebb67fdbdac",
     "showTitle": true,
     "title": "To extract our PDF,  we'll need to setup libraries in our nodes and define an extract function"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def parse_bytes_pypdf(raw_doc_contents_bytes: bytes):\n",
    "    try:\n",
    "        pdf = io.BytesIO(raw_doc_contents_bytes)\n",
    "        reader = PdfReader(pdf)\n",
    "        parsed_content = [page_content.extract_text() for page_content in reader.pages]\n",
    "        return \"\\n\".join(parsed_content)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Exception {e} has been thrown during parsing\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c16d492-7a97-4501-acc2-f2c56471102c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's start by extracting text from our PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c06b942e-e229-4ae9-a991-0b1bfd045858",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "with requests.get('https://github.com/databricks-demos/dbdemos-dataset/blob/main/llm/databricks-pdf-documentation/Databricks-Customer-360-ebook-Final.pdf?raw=true') as pdf:\n",
    "  doc = parse_bytes_pypdf(pdf.content)  \n",
    "  print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86bcd312-f1e1-46cd-9ed3-6d685c62712f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This looks great. We'll now wrap it with a text_splitter to avoid having too big pages, and create a Pandas UDF function to easily scale that across multiple nodes.\n",
    "\n",
    "*Note that our pdf text isn't clean. To make it nicer, we could imagine a few extra LLM-based pre-processing steps, asking to remove unrelevant content like the list of chapters to only keep the meat of the text.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "551d123e-f61f-433e-ae2c-12191775c6d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Document, set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Iterator\n",
    "\n",
    "# Reduce the arrow batch size as our PDF can be big in memory\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10)\n",
    "\n",
    "@pandas_udf(\"array<string>\")\n",
    "def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    #set llama2 as tokenizer to match our model size (will stay below gte 1024 limit)\n",
    "    set_global_tokenizer(\n",
    "      AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "    )\n",
    "    #Sentence splitter from llama_index to split on sentences\n",
    "    splitter = SentenceSplitter(chunk_size=500, chunk_overlap=10)\n",
    "    def extract_and_split(b):\n",
    "      txt = parse_bytes_pypdf(b)\n",
    "      if txt is None:\n",
    "        return []\n",
    "      nodes = splitter.get_nodes_from_documents([Document(text=txt)])\n",
    "      return [n.text for n in nodes]\n",
    "\n",
    "    for x in batch_iter:\n",
    "        yield x.apply(extract_and_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "887644a2-b755-4150-bc29-bbbb9fbb6106",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What's required for Pinecone Vector Database\n",
    "\n",
    "\n",
    "In this demo, we will show you how to use pinecone as your vector database\n",
    "\n",
    "We will first compute the embeddings of our chunks and save them as a Delta Lake table field as `array&ltfloat&gt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eb5ac71-0df6-4cf6-a51a-3860e22be10b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introducing Databricks BGE Embeddings Foundation Model endpoints\n",
    "\n",
    "<img src=\"https://github.com/prasadkona/databricks_demos/blob/main/images/llm-rag-full-pinecone-5.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "Foundation Models are provided by Databricks, and can be used out-of-the-box.\n",
    "\n",
    "Databricks supports several endpoint types to compute embeddings or evaluate a model:\n",
    "- A **foundation model endpoint**, provided by databricks (ex: llama2-70B, MPT...)\n",
    "- An **external endpoint**, acting as a gateway to an external model (ex: Azure OpenAI)\n",
    "- A **custom**, fined-tuned model hosted on Databricks model service\n",
    "\n",
    "Open the [Model Serving Endpoint page](/ml/endpoints) to explore and try the foundation models.\n",
    "\n",
    "For this demo, we will use the foundation model `BGE` (embeddings) and `llama2-70B` (chat). <br/><br/>\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/databricks-foundation-models.png?raw=true\" width=\"600px\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e29326c-0381-40aa-99f7-6d131ce4c8af",
     "showTitle": true,
     "title": "Using Databricks Foundation model BGE as embedding endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "# bge-large-en Foundation models are available using the /serving-endpoints/databricks-bge-large-en/invocations api. \n",
    "deploy_client = get_deploy_client(\"databricks\")\n",
    "\n",
    "## NOTE: if you change your embedding model here, make sure you change it in the query step too\n",
    "embeddings = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": [\"What is Apache Spark?\"]})\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65951d8-43d9-4e89-a5ff-a3bd7b51a11d",
     "showTitle": true,
     "title": "Create the final databricks_pdf_documentation table containing chunks and embeddings"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--Note that we need to enable Change Data Feed on the table to create the index\n",
    "CREATE TABLE IF NOT EXISTS databricks_pdf_documentation (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  url STRING,\n",
    "  content STRING,\n",
    "  metadata STRING,\n",
    "  embedding ARRAY <FLOAT>\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ef3d09-2fe4-4b5d-af3a-6e4c740953b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Computing the chunk embeddings and saving them to our Delta Table\n",
    "\n",
    "The last step is to now compute an embedding for all our documentation chunks. Let's create an udf to compute the embeddings using the foundation model endpoint.\n",
    "\n",
    "*Note that this part would typically be setup as a production-grade job, running as soon as a new documentation page is updated. <br/> This could be setup as a Delta Live Table pipeline to incrementally consume updates.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1bd7a54-00af-48af-bc7b-1366d491ce43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import pandas_udf,PandasUDFType, udf\n",
    "\n",
    "@pandas_udf(\"array<float>\")\n",
    "def get_embedding(contents: pd.Series) -> pd.Series:\n",
    "    import mlflow.deployments\n",
    "    deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "    def get_embeddings(batch):\n",
    "        #Note: this will gracefully fail if an exception is thrown during embedding creation (add try/except if needed) \n",
    "        response = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": batch})\n",
    "        return [e['embedding'] for e in response.data]\n",
    "\n",
    "    # Splitting the contents into batches of 150 items each, since the embedding model takes at most 150 inputs per request.\n",
    "    max_batch_size = 150\n",
    "    batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]\n",
    "\n",
    "    # Process each batch and collect the results\n",
    "    all_embeddings = []\n",
    "    for batch in batches:\n",
    "        all_embeddings += get_embeddings(batch.tolist())\n",
    "\n",
    "    return pd.Series(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d73e6674-3c2e-47d3-bbc9-af3bbc1eeb6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def create_metadata_json_string(original_col: pd.Series) -> pd.Series:\n",
    "    json_dict = {'original_doc': original_col}\n",
    "\n",
    "    return pd.Series(json.dumps(json_dict))\n",
    "\n",
    "#df = df.withColumn('json_col', create_metadata_json_string('input_col'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee610a9b-f1aa-4f5a-b6b3-eaa3ccd757b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UDF for embedding\n",
    "from pyspark.sql.types import *\n",
    "def get_embedding_for_string(text):\n",
    "    response = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": text})\n",
    "    e = response.data\n",
    "    return e[0]['embedding']\n",
    "\n",
    "get_embedding_for_string_udf = udf(get_embedding_for_string, ArrayType(FloatType()))\n",
    "print(get_embedding_for_string(\"What is a lakehouse ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b024b88f-01b8-4e62-9dd7-608dbe2344c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete checkpoint for the pdf_raw table streaming query\n",
    "#dbutils.fs.rm(f'{folder}/checkpoints/pdf_chunks_{catalog}_{db}', True)\n",
    "\n",
    "# Delete checkpoint for the databricks_documentation table streaming query\n",
    "#dbutils.fs.rm(f'{folder}/checkpoints/docs_chunks_{catalog}_{db}', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a90d47-8920-4bc6-820f-74c0d74cfa29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream.table('pdf_raw')\n",
    "      .withColumn(\"content\", F.explode(read_as_chunk(\"content\")))\n",
    "      .withColumn(\"embedding\", get_embedding(\"content\"))\n",
    "      .withColumn(\"metadata\", create_metadata_json_string(\"content\") )\n",
    "      #.selectExpr('path as url', 'content', 'embedding','metadata')\n",
    "      .selectExpr('path as url', 'content', 'embedding')\n",
    "  .writeStream\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", f'{folder}/checkpoints/pdf_chunks{catalog}_{db}')\n",
    "    .table('databricks_pdf_documentation').awaitTermination())\n",
    "\n",
    "#Let's also add our documentation web page from the simple demo (make sure you run the simple demo for it to work)\n",
    "if spark.catalog.tableExists(f'{catalog}.{db}.databricks_documentation'):\n",
    "  (spark.readStream.table('databricks_documentation')\n",
    "      .withColumn('embedding', get_embedding(\"content\"))\n",
    "      .withColumn(\"metadata\", create_metadata_json_string(\"content\") )\n",
    "      #.select('url', 'content', 'embedding','metadata')\n",
    "      .select('url', 'content', 'embedding')\n",
    "  .writeStream\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", f'{folder}/checkpoints/docs_chunks_{catalog}_{db}')\n",
    "    .table('databricks_pdf_documentation').awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49c2a8ae-bc88-472e-bd4b-2a431e34e891",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM databricks_pdf_documentation WHERE url like '%.pdf' limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af8bed2-4426-4f0c-b285-1ba600ac2b9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, IntegerType  \n",
    "schema = StructType([  \n",
    "    StructField(\"id\",StringType(),True),  \n",
    "    StructField(\"values\",ArrayType(FloatType()),True),  \n",
    "    StructField(\"namespace\",StringType(),True),  \n",
    "    StructField(\"metadata\", StringType(), True),  \n",
    "    StructField(\"sparse_values\", StructType([  \n",
    "        StructField(\"indices\", ArrayType(IntegerType(), False), False),  \n",
    "        StructField(\"values\", ArrayType(FloatType(), False), False)  \n",
    "    ]), True)  \n",
    "])  \n",
    "#embeddings_df = spark.createDataFrame(data=embeddings,schema=schema)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d5c2d1f-d011-4e70-a428-bd1462e4e80f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, struct, to_json\n",
    "from pyspark.sql.functions import encode\n",
    "\n",
    "df = spark.table('databricks_pdf_documentation')\\\n",
    "            .withColumn(\"metadata\", to_json(struct(col(\"content\"), col(\"url\"), col(\"id\"))))\\\n",
    "            .withColumn(\"namespace\", lit(\"dbdemo-namespace\")) \\\n",
    "            .withColumn(\"values\", col(\"embedding\")) \\\n",
    "            .withColumn(\"sparse_values\", lit(None)) \\\n",
    "            .select(\"id\", \"values\", \"namespace\", \"metadata\", \"sparse_values\")\n",
    "\n",
    "display(df.count())\n",
    "\n",
    "# Print the valid JSON\n",
    "display(df.limit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b310a44-39be-4f66-a5ca-4371d5298bff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If you dont know the embedding array size, use the below to determine the embedding array size.\n",
    "# The embedding array size varies based on the model used for converting a string to an embedding array\n",
    "# Note: Login to pinecone, Set the pinecone vector index to have the size of the embedding array \n",
    "\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "df2 = df.withColumn('array_col_len', size('values'))\n",
    "display(df2.limit(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a726d09a-6751-4da0-aa59-0b4dc40dc562",
     "showTitle": true,
     "title": "Initialize Pinecone client configs"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize pinecone variables\n",
    "\n",
    "api_key = dbutils.secrets.get(\"pinecone_secrets_scope\", \"PINECONE_API_KEY\")\n",
    "project_name = \"Starter\"\n",
    "index_name = \"dbdemo-index\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbec35f2-0a33-4f43-a2c9-1ed2b7bae15f",
     "showTitle": true,
     "title": "Write to pinecone"
    }
   },
   "outputs": [],
   "source": [
    "(  \n",
    "    df.write  \n",
    "    .option(\"pinecone.apiKey\", api_key) \n",
    "    .option(\"pinecone.indexName\", index_name)  \n",
    "    .format(\"io.pinecone.spark.pinecone.Pinecone\")  \n",
    "    .mode(\"append\")  \n",
    "    .save()  \n",
    ")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7bd6ca-7e40-4b6e-9249-9b2db3c72ef3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Our dataset is now ready! and is available for query via Pinecone\n",
    "\n",
    "Our dataset is now ready. We chunked the documentation page in small section, computed the embeddings and saved it as a Delta Lake table and ingested it into the Pinecone vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a95f42d6-8ef5-4a17-87c1-497261fc7bb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Searching for similar content\n",
    "\n",
    "Let's give it a try and search for similar content. Lets get the top n results \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93477f4d-bbf6-4122-b8e7-5c404f3de0aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# connect to pinecone index\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f0dfad-8867-4d25-b302-37eac45ed966",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf', 'There is a Ganglia dashboard at the cluster level, integrated partner \\napplications like Datadog  for monitoring streaming workloads, or even more open \\nsource options you can build using tools like Prometheus and Grafana. Each \\nhas advantages and disadvantages to consider around cost, performance, and \\nmaintenance requirements.\\nWhether you have low volumes of streaming workloads where interactions in the \\nUI are sufficient or have decided to invest in a more robust monitoring platform, \\nyou should know how to observe your production streaming workloads. Further \\n“Monitoring and Alerting” posts later in this series will contain a more thorough \\ndiscussion. In particular, we’ll see different measures on which to monitor \\nstreaming applications and then later take a deeper look at some of the tools \\nyou can leverage for observability.  \\nApplication optimization (Are resources being used effectively?  \\nThink “cost”)\\nThe next concern we have after deploying to production is “is my application \\nusing resources effectively?” As developers, we understand (or quickly learn) the \\ndistinction between working code and well-written code. Improving the way your \\ncode runs is usually very satisfying, but what ultimately matters is the overall \\ncost of running it. Cost considerations for Structured Streaming applications will \\nbe largely similar to those for other Spark applications. One notable difference \\nis that failing to optimize for production workloads can be extremely costly, \\nas these workloads are frequently “always-on” applications, and thus wasted \\nexpenditure can quickly compound. Because assistance with cost optimization is The Apache Spark Structured Streaming UI33\\nThe Big Book of Data Engineering – 2nd Edition\\nfrequently requested, a separate post in this series will address it. The key points \\nthat we’ll focus on will be efficiency of usage and sizing.\\nGetting the cluster sizing right is one of the most significant differences between \\nefficiency and wastefulness in streaming applications.', 0.587869525], ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/technical_guide_solving_common-data-challenges-for-startups-and-digital-native-businesses.pdf', '“Now our data engineers are able to build \\nreliable data pipelines that thread the needle on key topics such \\nas inventory management, allowing us to identify in near real-\\ntime what our trends are so we can figure out how to effectively \\nmove inventory.”  \\n \\nRead the full story here.CHALLENGE 01\\nCUSTOMER STORY: BUTCHERBOX12\\nCapture audit logs \\nUnity Catalog captures an audit log of actions \\nperformed against the metastore. To access audit \\nlogs for Unity Catalog events, you must enable and \\nconfigure audit logs for your account. Audit logs for \\neach workspace and account-level activities are \\ndelivered to your account. See how to configure audit \\nlogs and create a dashboard to analyze audit log data.\\nView data lineage  \\nYou can use Unity Catalog to capture runtime data \\nlineage across queries in any language executed on \\na Databricks cluster or SQL warehouse. Lineage can \\nbe visualized in Data Explorer in near real-time and \\nretrieved with the Databricks REST API. Lineage is \\naggregated across all workspaces attached to Unity \\nCatalog and captured down to the column level, and \\nincludes notebooks, workflows and dashboards related \\nto the query. To understand the requirements and how \\nto capture lineage data, see\\xa0Capture and view data \\nlineage with Unity Catalog.Set up secure data sharing  \\nDatabricks uses an open protocol called Delta Sharing  \\nto share data with other entities regardless of their \\ncomputing platforms. Delta Sharing is integrated with \\nUnity Catalog. Your data must be registered with Unity \\nCatalog to manage, govern, audit and track usage of the \\nshared data on the Lakehouse Platform.', 0.585427761], ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/technical_guide_solving_common-data-challenges-for-startups-and-digital-native-businesses.pdf', 'We’ll discuss this as a series of steps:\\n  1  Set up governance with Unity Catalog\\n  2  Grant secure access to the data\\n  3  Capture audit logs\\n  4  View data lineage\\n  5  Set up data sharingCHALLENGE 01\\n8\\nConfigure unified governance  \\nDatabricks recommends using catalogs to provide an easily searchable inventory of data, notebooks, dashboards and models. Often this \\nmeans that catalogs can correspond to software development environment scope, team or business unit. Unity Catalog manages how data \\nis secured, accessed and shared. Unity Catalog offers a single place to administer data access policies that apply across all workspace and \\npersonas and automatically captures user-level audit logs that record access to your data.  \\n \\nData stewards can securely grant access to a broad set of users to discover and analyze data at scale. These users can use a variety of \\nlanguages and tools, including SQL and Python, to create derivative data sets, models and dashboards that can be shared across teams.  \\nTo set up Unity Catalog for your organization,  \\nyou do the following:\\n1 Configure an S3 bucket and IAM role that \\nUnity Catalog can use to store and access \\ndata in your AWS account. \\n2 Create a metastore for each region in \\nwhich your organization operates, and \\nattach workspaces to the metastore. Each \\nworkspace will have the same view of the \\ndata you manage in Unity Catalog. 3\\n3 If you have a new account, add users, \\ngroups and service principals to your \\nDatabricks account.\\n4 Next, create and grant access to \\ncatalogs, schemas and tables.\\nFor complete setup instructions, see Get started using Unity Catalog.CHALLENGE 01\\n9\\nYou will notice that the hierarchy of primary data \\nobjects in Unity Catalog flows from metastore to table:How Unity  \\nCatalog works\\nMetastore is the top-level container for metadata.', 0.576542437], ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf', 'The majority of the \\nsuggestions in this post are relevant to both Structured Streaming Jobs and \\nDelta Live Tables (our flagship and fully managed ETL product that supports  \\nboth batch and streaming pipelines).\\n \\nAfter deployment\\nAfter the deployment of your streaming application, there are typically three \\nmain things you’ll want to know:\\n• How is my application running?\\n• Are resources being used efficiently?\\n• How do I manage any problems that come up?\\n \\nWe’ll start with an introduction to these topics, followed by a deeper dive later in \\nthis blog series.  Monitoring and instrumentation (How is my application running?)\\nStreaming workloads should be pretty much hands-off once deployed to \\nproduction. However, one thing that may sometimes come to mind is: “how is my \\napplication running?” Monitoring applications can take on different levels and \\nforms depending on:\\n• the metrics collected for your application (batch duration/latency, \\nthroughput, …)\\n• where you want to monitor the application from\\n \\nAt the simplest level, there is a streaming dashboard ( A Look at the New \\nStructured Streaming UI ) and built-in logging directly in the Spark UI that can be \\nused in a variety of situations.\\nThis is in addition to setting up failure alerts on jobs running streaming \\nworkloads.\\nIf you want more fine-grained metrics or to create custom actions based on \\nthese metrics as part of your code base, then the StreamingQueryListener is \\nbetter aligned with what you’re looking for.SECTION 2.5    \\nStreaming in Production: Collected Best Practices, Part 2\\nby ANGELA CHU and  TRISTEN WENTLING\\nJanuary 10, 202332\\nThe Big Book of Data Engineering – 2nd Edition\\nIf you want the Spark metrics to be reported (including machine level traces for \\ndrivers or workers) you should use the platform’s metrics sink .\\nAnother point to consider is where you want to surface these metrics for \\nobservability.', 0.574525177], ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/databricks_ultimate_gaming_data_guide_2023.pdf', 'Both Google AdMob and Unity Ads are great for banner \\nads, native ads, and rewarded video ads. Your role is to ensure \\nthat the data you’re collecting is fed back into the advertising \\nplatform to better serve targeted ads to your playerbase. To use a service like Databricks to manage the data needed \\nto provide real-time ad targeting in your application, you can \\nfollow the below steps:\\n1. Collect and store player data: Collect data on player \\nbehavior, preferences, and demographics, and store it in \\na data lake using Databricks. Popular analytics tools such \\nas Google Analytics or Mixpanel can be integrated into \\nthe game to collect data on player behavior. These tools, \\njust like tracking website traffic, can track in-game events, \\nprovide insights on player behavior and demographics.. \\nand they give you access to detailed reports and \\ndashboards. Another option is to build in-house tracking \\nsystems to collect data on player behavior - logging \\nevents, e.g in-game purchases or player actions, activities \\nsuch as “at which level does a player quit playing” and \\nstoring this in a database for analysis. The downside of \\nbuilding in-house tracking systems is you will need to host \\nand maintain your own logging servers.\\n2. Prepare the data:  Use Databricks to clean, transform, \\nand prepare the player data for analysis. This may \\ninclude aggregating data from multiple sources, removing \\nduplicates and outliers, and transforming the data into a \\nformat suitable for analysis.\\n3. Analyze the data: Use Databricks’ built-in machine \\nlearning and data analytics capabilities to analyze the \\nplayer data and identify patterns and trends.\\n4. Create audience segments:  Based on the analysis, \\nuse Databricks to create audience segments based on \\ncommon characteristics such as interests, behaviors, \\nand preferences.\\n5.', 0.57229358]]\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I track billing usage on my workspaces?\"\n",
    "\n",
    "# create the query embedding\n",
    "xq = get_embedding_for_string(question)\n",
    "\n",
    "# query pinecone the top 5 most similar results\n",
    "query_response = index.query(\n",
    "    namespace='dbdemo-namespace',\n",
    "    top_k=5,\n",
    "    include_values=True,\n",
    "    include_metadata=True,\n",
    "    vector=xq\n",
    ")\n",
    "\n",
    "#print(query_response)\n",
    "\n",
    "query_response_docs = []\n",
    "for match in query_response['matches']:\n",
    "    query_response_docs.append([match['metadata']['url'],match['metadata']['content'],match['score']])\n",
    "\n",
    "print(query_response_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d56dab3c-d1d6-4d79-8972-95ea21578994",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Next step: Deploy our chatbot model with RAG\n",
    "\n",
    "We've seen how Databricks Lakehouse AI makes it easy to ingest and prepare your documents, and write to Pinecone vector database.\n",
    "\n",
    "This simplifies and accelerates your data projects so that you can focus on the next step: creating your realtime chatbot endpoint with well-crafted prompt augmentation.\n",
    "\n",
    "Open the [02-Advanced-Chatbot-Chain]($./02-Advanced-Chatbot-Chain) notebook to create and deploy a chatbot endpoint."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 634720160573271,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "1.0 - PDF-Advanced-Data-Preparation - Using Pinecone",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
