{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "778524dc-cde0-4eab-b79c-0fb733eb1c69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2/ Advanced chatbot with message history and filter using Langchain\n",
    "\n",
    "<img src=\"https://github.com/prasadkona/databricks_demos/blob/main/images/llm-rag-full-pinecone-0i.png?raw=true\" style=\"float: right; margin-left: 10px\"  width=\"900px;\">\n",
    "\n",
    "Data is now available on the Pinecone vector database!\n",
    "\n",
    "Let's now create a more advanced langchain model to perform RAG.\n",
    "\n",
    "We will improve our langchain model with the following:\n",
    "\n",
    "- Build a complete chain supporting a chat history, using llama 2 input style\n",
    "- Add a filter to only answer Databricks-related questions\n",
    "- Compute the embeddings with Databricks BGE models within our chain to query the Pinecone vector database\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1444828305810485&notebook=02-Deploy-RAG-Chatbot-Model&demo_name=chatbot-rag-llm&event=VIEW\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c8677f-bd35-4386-ab28-4714d9410a79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U mlflow==2.15.1 pinecone-client==5.0.1 langchain-pinecone==0.1.3 langchain==0.2.0 databricks-sdk==0.30.0 langchain-community==0.2.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f54d054-9785-40e4-8d8b-85440077c90a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import mlflow\n",
    "import langchain\n",
    "\n",
    "# url used to send the request to your model from the serverless endpoint\n",
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "pinecone_index_name = \"dbdemo-index\"\n",
    "pinecone_namespace = 'dbdemo-namespace'\n",
    "pinecone_api_key = dbutils.secrets.get(\"pinecone_secrets_scope\", \"PINECONE_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = dbutils.secrets.get(\"pinecone_secrets_scope\", \"PINECONE_API_KEY\")\n",
    "#os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(\"pinecone_secrets_scope\", \"DATABRICKS_TOKEN\")\n",
    "#pinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "\n",
    "catalog = \"prasad_kona_dev\"\n",
    "db = \"rag_chatbot_prasad_kona\"\n",
    "\n",
    "# Set a debug flag\n",
    "debug_flag = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d22b670-c5b4-4b0d-a2b5-a4cf981d2816",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Register the chatbot model to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df5983e-83c1-4633-8c3e-2af03484aca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain notebook path: /Workspace/Users/prasad.kona@databricks.com/dbdemos/isv-chatbot-rag-llm-v20240108/rag_with_pinecone/2.1 - Advanced-Chatbot-Chain - Using Pinecone\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Specify the full path to the chain notebook\n",
    "chain_notebook_file = \"2.1 - Advanced-Chatbot-Chain - Using Pinecone\"\n",
    "chain_notebook_path = os.path.join(os.getcwd(), chain_notebook_file)\n",
    "\n",
    "print(f\"Chain notebook path: {chain_notebook_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3690572a-2378-4c81-9222-fa9b74e7a799",
     "showTitle": true,
     "title": "Provide the signature for the chain model"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "# Provide an example of the input schema that is used to set the MLflow model's signature\n",
    "\n",
    "#print(f'Testing with relevant history and question...')\n",
    "dialog = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is Apache Spark?\"}, \n",
    "        {\"role\": \"assistant\", \"content\": \"Apache Spark is an open-source data processing engine that is widely used in big data analytics.\"}, \n",
    "        {\"role\": \"user\", \"content\": \"Does it support streaming?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Yes.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me more about it's capabilities.\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "dialog_output = {'result': \"Delta Lake is an open-source storage layer that brings ACID transactions, scalable metadata handling, and unifies batch and streaming processing. It's built on top of Apache Parquet format, providing features like schema enforcement, data versioning (Time Travel), and scalable metadata handling. Delta Lake supports both batch and streaming workloads, making it a unified solution for big data processing. It also offers features like ACID transactions and schema evolution, which are crucial for maintaining data integrity and handling continuously changing data. Delta Lake is designed to handle petabyte-scale tables with billions of partitions and files, making it suitable for large-scale data processing.\", 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/building-reliable-data-lakes-at-scale-with-delta-lake.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/building-reliable-data-lakes-at-scale-with-delta-lake.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/The-Delta-Lake-Series-Lakehouse-012921.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf']}\n",
    "\n",
    "input_example = {\n",
    "   \"messages\": [\n",
    "       {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": \"How does billing work on Databricks?\",\n",
    "       }\n",
    "   ]\n",
    "}\n",
    "\n",
    "output_example = {'result': \"Databricks operates on a pay-as-you-go model, where you are billed based on the usage of cloud resources. The cost depends on the type and duration of cloud resources you use, such as compute instances and storage. You can monitor your usage and costs through the Databricks platform. For more specific billing details, I would recommend checking Databricks' official documentation or contacting their support.\", 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/EB-Ingesting-Data-FINAL.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-and-ai-use-cases-for-the-public-sector.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-and-ai-use-cases-for-the-public-sector.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/Databricks-Customer-360-ebook-Final.pdf']}\n",
    "\n",
    "signature = infer_signature(input_example, output_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c958d84-2fca-4d49-9c17-64cfafb7aa9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = f\"{catalog}.{db}.rag_with_pinecone_model\"\n",
    "\n",
    "with mlflow.start_run():\n",
    "    signature = infer_signature(input_example, output_example)\n",
    "    logged_chain_info = mlflow.langchain.log_model(\n",
    "        lc_model=chain_notebook_path,\n",
    "        artifact_path=\"chain\",\n",
    "        registered_model_name=model_name,\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        example_no_conversion=True, # required to allow the schema to work\n",
    "        extra_pip_requirements=[ \n",
    "          \"mlflow==\" + mlflow.__version__,\n",
    "          \"langchain==0.2.0\" ,\n",
    "          \"pinecone-client==5.0.1\",\n",
    "          \"langchain-pinecone==0.1.3\",\n",
    "          \"langchain-community==0.2.0\"\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c5796d3-d7f3-4a3e-be35-8a7ccb28ffa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'runs:/ef72ffcec9cf466ba2efe9ec83804abb/chain'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logged_chain_info.model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18bc8986-741d-468e-b9ea-a13c7f07d105",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's try loading our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de409a01-79b0-494e-8a5b-9eae64fc75c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'runs:/ef72ffcec9cf466ba2efe9ec83804abb/chain'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logged_chain_info.model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "177c8079-96eb-4353-8d76-280838b559be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcc1f5224b24eef9fe5313d48463d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'result': 'Apache Spark is a powerful data processing engine that supports various capabilities, making it suitable for big data analytics. Some of its key features include:\\n\\n1. **Streaming support**: Spark allows for streaming data processing, enabling real-time data analysis and decision-making.\\n2. **Scalable metadata handling**: Delta Lake, a key component of Databricks, stores metadata information in a transaction log instead of a metastore. This allows for efficient listing of files in large directories and reading data.\\n3. **Data versioning and time travel**: Delta Lake enables users to read previous snapshots of a table or directory. This feature is useful for reproducing experiments, reports, and reverting a table to its older versions if needed.\\n4. **Unified batch and streaming sink**: Apart from batch writes, Delta Lake can also be used as an efficient streaming sink with Apache Sparkâ€™s structured streaming. This enables near real-time analytics use cases without maintaining a complicated streaming and batch pipeline.\\n5. **Record update and deletion',\n",
       " 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/030521-2-The-Delta-Lake-Series-Complete-Collection.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/030521-2-The-Delta-Lake-Series-Complete-Collection.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "model.invoke(dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de681d3-0fad-4603-a9ea-7c3f5987c2f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'result': 'Billing on Databricks is based on usage and is typically charged to the cloud service provider account where the Databricks workspace is hosted. The billing is usually usage-based, meaning you only pay for the resources you use. This can lead to lower total cost of ownership compared to legacy Hadoop systems and can help reduce premiums for customers and lower loss ratios in insurance use cases. The serverless data plane network infrastructure is managed by Databricks in a Databricks cloud service provider account and shared among customers, with additional network boundaries between workspaces and between clusters.',\n",
       " 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/technical_guide_solving_common-data-challenges-for-startups-and-digital-native-businesses.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/The-Data-Teams-Guide-to-the-DB-Lakehouse-Platform.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/databricks_ebook_insurance_v10.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/the-big-book-of-mlops-v10-072023.pdf']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af050ec6-8f22-4384-99a3-fad7f4dcbea5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Deploying our Chat Model as a Serverless Model Endpoint \n",
    "\n",
    "Our model is saved in Unity Catalog. The last step is to deploy it as a Model Serving.\n",
    "\n",
    "We'll then be able to sending requests from our assistant frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90c90af0-10e3-4db6-ba87-8ba01dadf8af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_model_version(model_name):\n",
    "    from mlflow import MlflowClient\n",
    "    mlflow_client = MlflowClient()\n",
    "    latest_version = 1\n",
    "    for mv in mlflow_client.search_model_versions(f\"name='{model_name}'\"):\n",
    "        version_int = int(mv.version)\n",
    "        if version_int > latest_version:\n",
    "            latest_version = version_int\n",
    "    return latest_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e422bfaa-4937-400f-b465-5fa1ca01f070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5426752282b4e00a943fe7ae378e808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'result': \"Apache Spark is a powerful data processing engine that supports various capabilities, including:\\n\\n1. SQL Queries: Spark SQL allows relational processing with improved performance, and it can be used with SQL or through APIs in Python, Scala, and Java.\\n2. Streaming Data: Spark Streaming enables scalable and fault-tolerant processing of live data streams, which can be integrated with a wide range of sources.\\n3. Machine Learning: MLlib is Spark's distributed machine learning library, which provides various machine learning algorithms, including classification, regression, clustering, and collaborative filtering.\\n4. Graph Processing: GraphX is Spark's API for graph-parallel computation, which provides a set of fundamental operators for manipulating graphs and a library of common graph algorithms.\\n5. SparkR: SparkR is an R package that provides a light-weight frontend to use Spark from R, enabling data scientists to analyze large datasets and interact with data stored in various sources.\\n\\nThese capabilities make Spark a versatile tool for various data processing and analytics tasks,\",\n",
       " 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/building-reliable-data-lakes-at-scale-with-delta-lake.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/030521-2-The-Delta-Lake-Series-Complete-Collection.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/compact-guide-to-large-language-models.pdf']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlflow.langchain.load_model(\"models:/\"+model_name+\"/\"+str(get_latest_model_version(model_name)))\n",
    "model.invoke(dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70750c35-3544-41df-8965-91c27394b5bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "latest_model_version = get_latest_model_version(model_name)\n",
    "print(latest_model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e3145a-b0fc-49f7-bbda-3e99dad7f33a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create or update serving endpoint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedModelInput, ServedModelInputWorkloadSize\n",
    "import requests\n",
    "\n",
    "serving_endpoint_name = \"pinecone_rag_chain\"\n",
    "latest_model_version = get_latest_model_version(model_name)\n",
    "\n",
    "databricks_api_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "\n",
    "w = WorkspaceClient()\n",
    "endpoint_config = EndpointCoreConfigInput(\n",
    "    name=serving_endpoint_name,\n",
    "    served_models=[\n",
    "        ServedModelInput(\n",
    "            model_name=model_name,\n",
    "            model_version=latest_model_version,\n",
    "            workload_size=ServedModelInputWorkloadSize.SMALL,\n",
    "            scale_to_zero_enabled=True,\n",
    "            environment_vars={\n",
    "                \"PINECONE_API_KEY\": \"{{secrets/prasad_kona/PINECONE_API_KEY}}\",\n",
    "                \"DATABRICKS_TOKEN\": \"{{secrets/dbdemos/rag_sp_token}}\",\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "existing_endpoint = next(\n",
    "    (e for e in w.serving_endpoints.list() if e.name == serving_endpoint_name), None\n",
    ")\n",
    "serving_endpoint_url = f\"{host}/ml/endpoints/{serving_endpoint_name}\"\n",
    "if existing_endpoint == None:\n",
    "    print(f\"Creating the endpoint {serving_endpoint_url}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.create_and_wait(name=serving_endpoint_name, config=endpoint_config)\n",
    "else:\n",
    "    print(f\"Updating the endpoint {serving_endpoint_url} to version {latest_model_version}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.update_config_and_wait(served_models=endpoint_config.served_models, name=serving_endpoint_name)\n",
    "    \n",
    "displayHTML(f'Your Model Endpoint Serving is now available. Open the <a href=\"/ml/endpoints/{serving_endpoint_name}\">Model Serving Endpoint page</a> for more details.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3414656-1ab8-4f34-afe1-08d261c311aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Our endpoint is now deployed! You can search endpoint name on the [Serving Endpoint UI](#/mlflow/endpoints) and visualize its performance!\n",
    "\n",
    "Let's run a REST query to try it in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7219e62c-af9d-473c-946f-a7e5ce925925",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest_model_version=3\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "w = WorkspaceClient()\n",
    "serving_endpoint_name = \"pinecone_rag_chain\"\n",
    "latest_model_version = get_latest_model_version(model_name)\n",
    "print(\"latest_model_version=\"+str(latest_model_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19580ada-21bd-418c-8240-f17f1649922a",
     "showTitle": true,
     "title": "Let's try to send a query to our chatbot"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 'Yes, Apache Spark supports streaming through Spark Structured Streaming, which is a scalable and fault-tolerant stream processing engine. It provides an easy-to-use API for creating continuous, real-time data pipelines.', 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf']}\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk.service.serving import DataframeSplitInput\n",
    "\n",
    "test_dialog = DataframeSplitInput(\n",
    "    columns=[\"messages\"],\n",
    "    data=[\n",
    "        \n",
    "            {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"What is Apache Spark?\"},\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": \"Apache Spark is an open-source data processing engine that is widely used in big data analytics.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": \"Does it support streaming?\"},\n",
    "                ]\n",
    "            }\n",
    "        \n",
    "    ],\n",
    ")\n",
    "answer = w.serving_endpoints.query(serving_endpoint_name, dataframe_split=test_dialog)\n",
    "print(answer.predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee721c2c-8d38-491e-8e27-c296fa015d1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 'Billing on Databricks is usage-based. Customers are charged according to the number of Databricks Units (DBUs) consumed. A DBU is a unit of measure for the processing power used in Databricks, which includes the use of compute resources and managed services. The cost per DBU depends on the type of Databricks Runtime and the cloud service provider. Databricks provides detailed usage reports to help customers monitor and manage their costs.', 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/technical_guide_solving_common-data-challenges-for-startups-and-digital-native-businesses.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/The-Data-Teams-Guide-to-the-DB-Lakehouse-Platform.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/databricks_ebook_insurance_v10.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/the-big-book-of-mlops-v10-072023.pdf']}\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk.service.serving import DataframeSplitInput\n",
    "\n",
    "test_dialog = DataframeSplitInput(\n",
    "    columns=[\"messages\"],\n",
    "    data=[\n",
    "        \n",
    "            {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"How does billing work on Databricks?\"},\n",
    "                    \n",
    "                ]\n",
    "            }\n",
    "        \n",
    "    ],\n",
    ")\n",
    "answer = w.serving_endpoints.query(serving_endpoint_name, dataframe_split=test_dialog)\n",
    "print(answer.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbfdcb71-979f-4e97-9abf-8b47d1c64ab9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Congratulations! You have deployed your RAG application with Pinecone!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2.2 - Advanced-Chatbot-Driver - Using Pinecone",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
