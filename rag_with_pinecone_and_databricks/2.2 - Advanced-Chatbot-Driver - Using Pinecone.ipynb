{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "778524dc-cde0-4eab-b79c-0fb733eb1c69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2/ Advanced chatbot with message history and filter using Langchain\n",
    "\n",
    "<img src=\"https://github.com/prasadkona/databricks_demos/blob/main/images/llm-rag-full-pinecone-0i.png?raw=true\" style=\"float: right; margin-left: 10px\"  width=\"900px;\">\n",
    "\n",
    "Data is now available on the Pinecone vector database!\n",
    "\n",
    "Let's now create a more advanced langchain model to perform RAG.\n",
    "\n",
    "We will improve our langchain model with the following:\n",
    "\n",
    "- Build a complete chain supporting a chat history, using llama 2 input style\n",
    "- Add a filter to only answer Databricks-related questions\n",
    "- Compute the embeddings with Databricks BGE models within our chain to query the Pinecone vector database\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1444828305810485&notebook=02-Deploy-RAG-Chatbot-Model&demo_name=chatbot-rag-llm&event=VIEW\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c8677f-bd35-4386-ab28-4714d9410a79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow==2.15.1\n  Obtaining dependency information for mlflow==2.15.1 from https://files.pythonhosted.org/packages/19/7a/7d5594ddcaaff7a92caed1d7822cfe52ed01fe06c94b4ad88bcfef579c32/mlflow-2.15.1-py3-none-any.whl.metadata\n  Using cached mlflow-2.15.1-py3-none-any.whl.metadata (29 kB)\nCollecting pinecone-client==5.0.1\n  Obtaining dependency information for pinecone-client==5.0.1 from https://files.pythonhosted.org/packages/55/d0/c64336b8f76e63296d04b885c545c0872ff070e6b2bc725dd0ff3ae681dc/pinecone_client-5.0.1-py3-none-any.whl.metadata\n  Using cached pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\nCollecting langchain-pinecone==0.1.3\n  Obtaining dependency information for langchain-pinecone==0.1.3 from https://files.pythonhosted.org/packages/8f/34/0389b167e99186b1f73bf25d4a63457782bce7fb43518d57612a0a0c0546/langchain_pinecone-0.1.3-py3-none-any.whl.metadata\n  Using cached langchain_pinecone-0.1.3-py3-none-any.whl.metadata (1.7 kB)\nCollecting langchain==0.2.0\n  Obtaining dependency information for langchain==0.2.0 from https://files.pythonhosted.org/packages/6a/3a/66115e1986e21f7a4a855df2c769c35bea84ca26b31f4e08951a364bb983/langchain-0.2.0-py3-none-any.whl.metadata\n  Using cached langchain-0.2.0-py3-none-any.whl.metadata (13 kB)\nCollecting databricks-sdk==0.30.0\n  Obtaining dependency information for databricks-sdk==0.30.0 from https://files.pythonhosted.org/packages/ad/14/005d6904ffbf477e67913f6480b22522e583ae25160d1fce30d15c1dfa94/databricks_sdk-0.30.0-py3-none-any.whl.metadata\n  Using cached databricks_sdk-0.30.0-py3-none-any.whl.metadata (37 kB)\nCollecting langchain-community==0.2.0\n  Obtaining dependency information for langchain-community==0.2.0 from https://files.pythonhosted.org/packages/1b/ad/59d9f88057c29d0a5d9ed786358bbbc8797bda347f3f007d51a651d2613a/langchain_community-0.2.0-py3-none-any.whl.metadata\n  Using cached langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\nCollecting mlflow-skinny==2.15.1 (from mlflow==2.15.1)\n  Obtaining dependency information for mlflow-skinny==2.15.1 from https://files.pythonhosted.org/packages/ec/a1/3812743e5dd83317d0469a46d737f0ab5c084fecfecc03a1ac8a7e7ec0d8/mlflow_skinny-2.15.1-py3-none-any.whl.metadata\n  Using cached mlflow_skinny-2.15.1-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: Flask<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow==2.15.1) (2.2.5)\nCollecting alembic!=1.10.0,<2 (from mlflow==2.15.1)\n  Obtaining dependency information for alembic!=1.10.0,<2 from https://files.pythonhosted.org/packages/df/ed/c884465c33c25451e4a5cd4acad154c29e5341e3214e220e7f3478aa4b0d/alembic-1.13.2-py3-none-any.whl.metadata\n  Using cached alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\nCollecting docker<8,>=4.0.0 (from mlflow==2.15.1)\n  Obtaining dependency information for docker<8,>=4.0.0 from https://files.pythonhosted.org/packages/e3/26/57c6fb270950d476074c087527a558ccb6f4436657314bfb6cdf484114c4/docker-7.1.0-py3-none-any.whl.metadata\n  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting graphene<4 (from mlflow==2.15.1)\n  Obtaining dependency information for graphene<4 from https://files.pythonhosted.org/packages/24/70/96f6027cdfc9bb89fc07627b615cb43fb1c443c93498412beaeaf157e9f1/graphene-3.3-py2.py3-none-any.whl.metadata\n  Using cached graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.11/site-packages (from mlflow==2.15.1) (3.4.1)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow==2.15.1) (3.7.2)\nRequirement already satisfied: numpy<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow==2.15.1) (1.23.5)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.11/site-packages (from mlflow==2.15.1) (1.5.3)\nRequirement already satisfied: pyarrow<16,>=4.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow==2.15.1) (14.0.1)\nCollecting querystring-parser<2 (from mlflow==2.15.1)\n  Obtaining dependency information for querystring-parser<2 from https://files.pythonhosted.org/packages/88/6b/572b2590fd55114118bf08bde63c0a421dcc82d593700f3e2ad89908a8a9/querystring_parser-1.2.4-py2.py3-none-any.whl.metadata\n  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow==2.15.1) (1.3.0)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow==2.15.1) (1.11.1)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow==2.15.1) (1.4.39)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.11/site-packages (from mlflow==2.15.1) (3.1.2)\nRequirement already satisfied: gunicorn<23 in /databricks/python3/lib/python3.11/site-packages (from mlflow==2.15.1) (20.1.0)\nRequirement already satisfied: certifi>=2019.11.17 in /databricks/python3/lib/python3.11/site-packages (from pinecone-client==5.0.1) (2023.7.22)\nCollecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client==5.0.1)\n  Obtaining dependency information for pinecone-plugin-inference<2.0.0,>=1.0.3 from https://files.pythonhosted.org/packages/0d/b7/0d57cad06545ac8fbb7a362dddaff01b0ecfe6e47c135345e94b3d8ab2ca/pinecone_plugin_inference-1.0.3-py3-none-any.whl.metadata\n  Using cached pinecone_plugin_inference-1.0.3-py3-none-any.whl.metadata (2.2 kB)\nCollecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client==5.0.1)\n  Obtaining dependency information for pinecone-plugin-interface<0.0.8,>=0.0.7 from https://files.pythonhosted.org/packages/3b/1d/a21fdfcd6d022cb64cef5c2a29ee6691c6c103c4566b41646b080b7536a5/pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata\n  Using cached pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: tqdm>=4.64.1 in /databricks/python3/lib/python3.11/site-packages (from pinecone-client==5.0.1) (4.65.0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /databricks/python3/lib/python3.11/site-packages (from pinecone-client==5.0.1) (4.10.0)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.11/site-packages (from pinecone-client==5.0.1) (1.26.16)\nCollecting aiohttp<4.0.0,>=3.9.5 (from langchain-pinecone==0.1.3)\n  Obtaining dependency information for aiohttp<4.0.0,>=3.9.5 from https://files.pythonhosted.org/packages/2a/92/006690c31b830acbae09d2618e41308fe4c81c0679b3b33a3af859e0b7bf/aiohttp-3.10.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Using cached aiohttp-3.10.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\nRequirement already satisfied: langchain-core<0.3,>=0.1.52 in /databricks/python3/lib/python3.11/site-packages (from langchain-pinecone==0.1.3) (0.1.52)\nRequirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.2.0) (6.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.2.0) (0.6.7)\nCollecting langchain-core<0.3,>=0.1.52 (from langchain-pinecone==0.1.3)\n  Obtaining dependency information for langchain-core<0.3,>=0.1.52 from https://files.pythonhosted.org/packages/4b/e8/bd5d7e12dee9c7b6e5d11fcaf02f36027dbc371cc87cb5c15e2a581b7c05/langchain_core-0.2.33-py3-none-any.whl.metadata\n  Using cached langchain_core-0.2.33-py3-none-any.whl.metadata (6.2 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.0)\n  Obtaining dependency information for langchain-text-splitters<0.3.0,>=0.2.0 from https://files.pythonhosted.org/packages/06/76/9e0ca1b8881f64bf927f2205bf6c43a085c04646a71d911b3c05d76e90bb/langchain_text_splitters-0.2.2-py3-none-any.whl.metadata\n  Using cached langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.2.0) (0.1.63)\nRequirement already satisfied: pydantic<3,>=1 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.2.0) (1.10.6)\nRequirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.2.0) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /databricks/python3/lib/python3.11/site-packages (from langchain==0.2.0) (8.2.2)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk==0.30.0) (2.21.0)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow==2.15.1) (5.4.0)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow==2.15.1) (8.0.4)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow==2.15.1) (2.2.1)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow==2.15.1) (0.4)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow==2.15.1) (3.1.27)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow==2.15.1) (6.0.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow==2.15.1) (1.25.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow==2.15.1) (1.25.0)\nRequirement already satisfied: packaging<25 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow==2.15.1) (23.2)\nRequirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow==2.15.1) (4.24.1)\nRequirement already satisfied: pytz<2025 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow==2.15.1) (2022.7)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow==2.15.1) (0.4.2)\nCollecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.9.5->langchain-pinecone==0.1.3)\n  Obtaining dependency information for aiohappyeyeballs>=2.3.0 from https://files.pythonhosted.org/packages/18/b6/58ea188899950d759a837f9a58b2aee1d1a380ea4d6211ce9b1823748851/aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata\n  Using cached aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.5->langchain-pinecone==0.1.3) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.5->langchain-pinecone==0.1.3) (22.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.5->langchain-pinecone==0.1.3) (1.3.3)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.5->langchain-pinecone==0.1.3) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.5->langchain-pinecone==0.1.3) (1.8.1)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow==2.15.1) (1.2.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /databricks/python3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0) (3.21.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /databricks/python3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0) (0.9.0)\nRequirement already satisfied: Werkzeug>=2.2.2 in /databricks/python3/lib/python3.11/site-packages (from Flask<4->mlflow==2.15.1) (2.2.3)\nRequirement already satisfied: itsdangerous>=2.0 in /databricks/python3/lib/python3.11/site-packages (from Flask<4->mlflow==2.15.1) (2.0.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk==0.30.0) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk==0.30.0) (4.9)\nRequirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth~=2.0->databricks-sdk==0.30.0) (1.16.0)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow==2.15.1)\n  Obtaining dependency information for graphql-core<3.3,>=3.1 from https://files.pythonhosted.org/packages/f8/39/e5143e7ec70939d2076c1165ae9d4a3815597019c4d797b7f959cf778600/graphql_core-3.2.3-py3-none-any.whl.metadata\n  Using cached graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow==2.15.1)\n  Obtaining dependency information for graphql-relay<3.3,>=3.1 from https://files.pythonhosted.org/packages/74/16/a4cf06adbc711bd364a73ce043b0b08d8fa5aae3df11b6ee4248bcdad2e0/graphql_relay-3.2.0-py3-none-any.whl.metadata\n  Using cached graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nCollecting aniso8601<10,>=8 (from graphene<4->mlflow==2.15.1)\n  Obtaining dependency information for aniso8601<10,>=8 from https://files.pythonhosted.org/packages/e3/04/e97c12dc034791d7b504860acfcdd2963fa21ae61eaca1c9d31245f812c3/aniso8601-9.0.1-py2.py3-none-any.whl.metadata\n  Using cached aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.11/site-packages (from gunicorn<23->mlflow==2.15.1) (68.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.11/site-packages (from Jinja2<4,>=2.11->mlflow==2.15.1) (2.1.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /databricks/python3/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.3) (1.33)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.0)\n  Obtaining dependency information for langsmith<0.2.0,>=0.1.17 from https://files.pythonhosted.org/packages/b5/af/b82590f2830559a0c2438e96d110bfeb002b2687c08a57bf117d9ea6b714/langsmith-0.1.101-py3-none-any.whl.metadata\n  Using cached langsmith-0.1.101-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: httpx<1,>=0.23.0 in /databricks/python3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /databricks/python3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (3.10.6)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.15.1) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.15.1) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.15.1) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.15.1) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.15.1) (9.4.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.15.1) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.15.1) (2.8.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.2.0) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.2.0) (3.4)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow==2.15.1) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow==2.15.1) (2.2.0)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.11/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.15.1) (2.0.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.15.1->mlflow==2.15.1) (4.0.11)\nRequirement already satisfied: anyio in /databricks/python3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (3.5.0)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (1.0.5)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (1.2.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (0.14.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.11/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow-skinny==2.15.1->mlflow==2.15.1) (3.11.0)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.3) (3.0.0)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.11/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.15.1->mlflow==2.15.1) (1.2.14)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /databricks/python3/lib/python3.11/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.15.1->mlflow==2.15.1) (0.46b0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk==0.30.0) (0.4.8)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.2.0) (0.4.3)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.15.1->mlflow==2.15.1) (1.14.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.15.1->mlflow==2.15.1) (5.0.0)\nUsing cached mlflow-2.15.1-py3-none-any.whl (26.3 MB)\nUsing cached pinecone_client-5.0.1-py3-none-any.whl (244 kB)\nUsing cached langchain_pinecone-0.1.3-py3-none-any.whl (10 kB)\nUsing cached langchain-0.2.0-py3-none-any.whl (973 kB)\nUsing cached databricks_sdk-0.30.0-py3-none-any.whl (538 kB)\nUsing cached langchain_community-0.2.0-py3-none-any.whl (2.1 MB)\nUsing cached mlflow_skinny-2.15.1-py3-none-any.whl (5.5 MB)\nUsing cached aiohttp-3.10.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\nUsing cached alembic-1.13.2-py3-none-any.whl (232 kB)\nUsing cached docker-7.1.0-py3-none-any.whl (147 kB)\nUsing cached graphene-3.3-py2.py3-none-any.whl (128 kB)\nUsing cached langchain_core-0.2.33-py3-none-any.whl (391 kB)\nUsing cached langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nUsing cached langsmith-0.1.101-py3-none-any.whl (148 kB)\nUsing cached pinecone_plugin_inference-1.0.3-py3-none-any.whl (117 kB)\nUsing cached pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\nUsing cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nUsing cached aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\nUsing cached aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\nUsing cached graphql_core-3.2.3-py3-none-any.whl (202 kB)\nUsing cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nInstalling collected packages: aniso8601, querystring-parser, pinecone-plugin-interface, graphql-core, aiohappyeyeballs, pinecone-plugin-inference, graphql-relay, docker, alembic, aiohttp, pinecone-client, langsmith, graphene, databricks-sdk, langchain-core, mlflow-skinny, langchain-text-splitters, langchain-pinecone, mlflow, langchain, langchain-community\n  Attempting uninstall: aiohttp\n    Found existing installation: aiohttp 3.8.5\n    Not uninstalling aiohttp at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85\n    Can't uninstall 'aiohttp'. No files were found to uninstall.\n  Attempting uninstall: langsmith\n    Found existing installation: langsmith 0.1.63\n    Not uninstalling langsmith at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85\n    Can't uninstall 'langsmith'. No files were found to uninstall.\n  Attempting uninstall: databricks-sdk\n    Found existing installation: databricks-sdk 0.20.0\n    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85\n    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.1.52\n    Not uninstalling langchain-core at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85\n    Can't uninstall 'langchain-core'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.13.1\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.0.2\n    Not uninstalling langchain-text-splitters at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85\n    Can't uninstall 'langchain-text-splitters'. No files were found to uninstall.\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.1.20\n    Not uninstalling langchain at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85\n    Can't uninstall 'langchain'. No files were found to uninstall.\n  Attempting uninstall: langchain-community\n    Found existing installation: langchain-community 0.0.38\n    Not uninstalling langchain-community at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85\n    Can't uninstall 'langchain-community'. No files were found to uninstall.\nSuccessfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 alembic-1.13.2 aniso8601-9.0.1 databricks-sdk-0.30.0 docker-7.1.0 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 langchain-0.2.0 langchain-community-0.2.0 langchain-core-0.2.33 langchain-pinecone-0.1.3 langchain-text-splitters-0.2.2 langsmith-0.1.101 mlflow-2.15.1 mlflow-skinny-2.15.1 pinecone-client-5.0.1 pinecone-plugin-inference-1.0.3 pinecone-plugin-interface-0.0.7 querystring-parser-1.2.4\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U mlflow==2.15.1 pinecone-client==5.0.1 langchain-pinecone==0.1.3 langchain==0.2.0 databricks-sdk==0.30.0 langchain-community==0.2.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f54d054-9785-40e4-8d8b-85440077c90a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import mlflow\n",
    "import langchain\n",
    "\n",
    "# url used to send the request to your model from the serverless endpoint\n",
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "pinecone_index_name = \"dbdemo-index\"\n",
    "pinecone_namespace = 'dbdemo-namespace'\n",
    "pinecone_api_key = dbutils.secrets.get(\"pinecone_secrets_scope\", \"PINECONE_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = dbutils.secrets.get(\"pinecone_secrets_scope\", \"PINECONE_API_KEY\")\n",
    "#os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(\"pinecone_secrets_scope\", \"DATABRICKS_TOKEN\")\n",
    "#pinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "\n",
    "catalog = \"prasad_kona_dev\"\n",
    "db = \"rag_chatbot_prasad_kona\"\n",
    "\n",
    "# Set a debug flag\n",
    "debug_flag = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d22b670-c5b4-4b0d-a2b5-a4cf981d2816",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Register the chatbot model to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df5983e-83c1-4633-8c3e-2af03484aca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain notebook path: /Workspace/Users/prasad.kona@databricks.com/dbdemos/isv-chatbot-rag-llm-v20240108/rag_with_pinecone/2.1 - Advanced-Chatbot-Chain - Using Pinecone\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Specify the full path to the chain notebook\n",
    "chain_notebook_file = \"2.1 - Advanced-Chatbot-Chain - Using Pinecone\"\n",
    "chain_notebook_path = os.path.join(os.getcwd(), chain_notebook_file)\n",
    "\n",
    "print(f\"Chain notebook path: {chain_notebook_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3690572a-2378-4c81-9222-fa9b74e7a799",
     "showTitle": true,
     "title": "Provide the signature for the chain model"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "# Provide an example of the input schema that is used to set the MLflow model's signature\n",
    "\n",
    "#print(f'Testing with relevant history and question...')\n",
    "dialog = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is Apache Spark?\"}, \n",
    "        {\"role\": \"assistant\", \"content\": \"Apache Spark is an open-source data processing engine that is widely used in big data analytics.\"}, \n",
    "        {\"role\": \"user\", \"content\": \"Does it support streaming?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Yes.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me more about it's capabilities.\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "dialog_output = {'result': \"Delta Lake is an open-source storage layer that brings ACID transactions, scalable metadata handling, and unifies batch and streaming processing. It's built on top of Apache Parquet format, providing features like schema enforcement, data versioning (Time Travel), and scalable metadata handling. Delta Lake supports both batch and streaming workloads, making it a unified solution for big data processing. It also offers features like ACID transactions and schema evolution, which are crucial for maintaining data integrity and handling continuously changing data. Delta Lake is designed to handle petabyte-scale tables with billions of partitions and files, making it suitable for large-scale data processing.\", 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/building-reliable-data-lakes-at-scale-with-delta-lake.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/building-reliable-data-lakes-at-scale-with-delta-lake.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/The-Delta-Lake-Series-Lakehouse-012921.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf']}\n",
    "\n",
    "input_example = {\n",
    "   \"messages\": [\n",
    "       {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": \"How does billing work on Databricks?\",\n",
    "       }\n",
    "   ]\n",
    "}\n",
    "\n",
    "output_example = {'result': \"Databricks operates on a pay-as-you-go model, where you are billed based on the usage of cloud resources. The cost depends on the type and duration of cloud resources you use, such as compute instances and storage. You can monitor your usage and costs through the Databricks platform. For more specific billing details, I would recommend checking Databricks' official documentation or contacting their support.\", 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/EB-Ingesting-Data-FINAL.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-and-ai-use-cases-for-the-public-sector.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-and-ai-use-cases-for-the-public-sector.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/Databricks-Customer-360-ebook-Final.pdf']}\n",
    "\n",
    "signature = infer_signature(input_example, output_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c958d84-2fca-4d49-9c17-64cfafb7aa9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/21 16:31:56 WARNING mlflow.models.utils: The model file uses magic commands which have been commented out. To ensure your code functions correctly, make sure that it does not rely on these magic commands for correctness.\n/databricks/python/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5dc61f61864e19a49dee1d04b68e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'prasad_kona_dev.rag_chatbot_prasad_kona.rag_with_pinecone_model' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99caef2908854fcf9ac5e2efa20fb711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '3' of model 'prasad_kona_dev.rag_chatbot_prasad_kona.rag_with_pinecone_model'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbdde589a7c4800905b3ee8c6b4b307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/21 16:32:17 INFO mlflow.tracking._tracking_service.client: 🏃 View run rare-stoat-99 at: e2-demo-field-eng.cloud.databricks.com/ml/experiments/634720160579851/runs/ef72ffcec9cf466ba2efe9ec83804abb.\n2024/08/21 16:32:17 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: e2-demo-field-eng.cloud.databricks.com/ml/experiments/634720160579851.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = f\"{catalog}.{db}.rag_with_pinecone_model\"\n",
    "\n",
    "with mlflow.start_run():\n",
    "    signature = infer_signature(input_example, output_example)\n",
    "    logged_chain_info = mlflow.langchain.log_model(\n",
    "        lc_model=chain_notebook_path,\n",
    "        artifact_path=\"chain\",\n",
    "        registered_model_name=model_name,\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        example_no_conversion=True, # required to allow the schema to work\n",
    "        extra_pip_requirements=[ \n",
    "          \"mlflow==\" + mlflow.__version__,\n",
    "          \"langchain==0.2.0\" ,\n",
    "          \"pinecone-client==5.0.1\",\n",
    "          \"langchain-pinecone==0.1.3\",\n",
    "          \"langchain-community==0.2.0\"\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c5796d3-d7f3-4a3e-be35-8a7ccb28ffa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'runs:/ef72ffcec9cf466ba2efe9ec83804abb/chain'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logged_chain_info.model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18bc8986-741d-468e-b9ea-a13c7f07d105",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's try loading our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de409a01-79b0-494e-8a5b-9eae64fc75c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'runs:/ef72ffcec9cf466ba2efe9ec83804abb/chain'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logged_chain_info.model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "177c8079-96eb-4353-8d76-280838b559be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcc1f5224b24eef9fe5313d48463d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'result': 'Apache Spark is a powerful data processing engine that supports various capabilities, making it suitable for big data analytics. Some of its key features include:\\n\\n1. **Streaming support**: Spark allows for streaming data processing, enabling real-time data analysis and decision-making.\\n2. **Scalable metadata handling**: Delta Lake, a key component of Databricks, stores metadata information in a transaction log instead of a metastore. This allows for efficient listing of files in large directories and reading data.\\n3. **Data versioning and time travel**: Delta Lake enables users to read previous snapshots of a table or directory. This feature is useful for reproducing experiments, reports, and reverting a table to its older versions if needed.\\n4. **Unified batch and streaming sink**: Apart from batch writes, Delta Lake can also be used as an efficient streaming sink with Apache Spark’s structured streaming. This enables near real-time analytics use cases without maintaining a complicated streaming and batch pipeline.\\n5. **Record update and deletion',\n",
       " 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/030521-2-The-Delta-Lake-Series-Complete-Collection.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/030521-2-The-Delta-Lake-Series-Complete-Collection.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "model.invoke(dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de681d3-0fad-4603-a9ea-7c3f5987c2f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'result': 'Billing on Databricks is based on usage and is typically charged to the cloud service provider account where the Databricks workspace is hosted. The billing is usually usage-based, meaning you only pay for the resources you use. This can lead to lower total cost of ownership compared to legacy Hadoop systems and can help reduce premiums for customers and lower loss ratios in insurance use cases. The serverless data plane network infrastructure is managed by Databricks in a Databricks cloud service provider account and shared among customers, with additional network boundaries between workspaces and between clusters.',\n",
       " 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/technical_guide_solving_common-data-challenges-for-startups-and-digital-native-businesses.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/The-Data-Teams-Guide-to-the-DB-Lakehouse-Platform.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/databricks_ebook_insurance_v10.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/the-big-book-of-mlops-v10-072023.pdf']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af050ec6-8f22-4384-99a3-fad7f4dcbea5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Deploying our Chat Model as a Serverless Model Endpoint \n",
    "\n",
    "Our model is saved in Unity Catalog. The last step is to deploy it as a Model Serving.\n",
    "\n",
    "We'll then be able to sending requests from our assistant frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90c90af0-10e3-4db6-ba87-8ba01dadf8af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_model_version(model_name):\n",
    "    from mlflow import MlflowClient\n",
    "    mlflow_client = MlflowClient()\n",
    "    latest_version = 1\n",
    "    for mv in mlflow_client.search_model_versions(f\"name='{model_name}'\"):\n",
    "        version_int = int(mv.version)\n",
    "        if version_int > latest_version:\n",
    "            latest_version = version_int\n",
    "    return latest_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e422bfaa-4937-400f-b465-5fa1ca01f070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5426752282b4e00a943fe7ae378e808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'result': \"Apache Spark is a powerful data processing engine that supports various capabilities, including:\\n\\n1. SQL Queries: Spark SQL allows relational processing with improved performance, and it can be used with SQL or through APIs in Python, Scala, and Java.\\n2. Streaming Data: Spark Streaming enables scalable and fault-tolerant processing of live data streams, which can be integrated with a wide range of sources.\\n3. Machine Learning: MLlib is Spark's distributed machine learning library, which provides various machine learning algorithms, including classification, regression, clustering, and collaborative filtering.\\n4. Graph Processing: GraphX is Spark's API for graph-parallel computation, which provides a set of fundamental operators for manipulating graphs and a library of common graph algorithms.\\n5. SparkR: SparkR is an R package that provides a light-weight frontend to use Spark from R, enabling data scientists to analyze large datasets and interact with data stored in various sources.\\n\\nThese capabilities make Spark a versatile tool for various data processing and analytics tasks,\",\n",
       " 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/building-reliable-data-lakes-at-scale-with-delta-lake.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/030521-2-The-Delta-Lake-Series-Complete-Collection.pdf',\n",
       "  'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/compact-guide-to-large-language-models.pdf']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlflow.langchain.load_model(\"models:/\"+model_name+\"/\"+str(get_latest_model_version(model_name)))\n",
    "model.invoke(dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70750c35-3544-41df-8965-91c27394b5bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "latest_model_version = get_latest_model_version(model_name)\n",
    "print(latest_model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e3145a-b0fc-49f7-bbda-3e99dad7f33a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the endpoint https://e2-demo-field-eng.cloud.databricks.com/ml/endpoints/pinecone_rag_chain to version 3, this will take a few minutes to package and deploy the endpoint...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTimeoutError\u001B[0m                              Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-634720160579868>, line 38\u001B[0m\n",
       "\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m     37\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpdating the endpoint \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mserving_endpoint_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m to version \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlatest_model_version\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, this will take a few minutes to package and deploy the endpoint...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 38\u001B[0m     w\u001B[38;5;241m.\u001B[39mserving_endpoints\u001B[38;5;241m.\u001B[39mupdate_config_and_wait(served_models\u001B[38;5;241m=\u001B[39mendpoint_config\u001B[38;5;241m.\u001B[39mserved_models, name\u001B[38;5;241m=\u001B[39mserving_endpoint_name)\n",
       "\u001B[1;32m     40\u001B[0m displayHTML(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYour Model Endpoint Serving is now available. Open the <a href=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/ml/endpoints/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mserving_endpoint_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m>Model Serving Endpoint page</a> for more details.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85/lib/python3.11/site-packages/databricks/sdk/service/serving.py:2747\u001B[0m, in \u001B[0;36mServingEndpointsAPI.update_config_and_wait\u001B[0;34m(self, name, auto_capture_config, served_entities, served_models, traffic_config, timeout)\u001B[0m\n",
       "\u001B[1;32m   2734\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate_config_and_wait\u001B[39m(\n",
       "\u001B[1;32m   2735\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m   2736\u001B[0m     name: \u001B[38;5;28mstr\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2741\u001B[0m     traffic_config: Optional[TrafficConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m   2742\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimedelta(minutes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m)) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ServingEndpointDetailed:\n",
       "\u001B[1;32m   2743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_config(auto_capture_config\u001B[38;5;241m=\u001B[39mauto_capture_config,\n",
       "\u001B[1;32m   2744\u001B[0m                               name\u001B[38;5;241m=\u001B[39mname,\n",
       "\u001B[1;32m   2745\u001B[0m                               served_entities\u001B[38;5;241m=\u001B[39mserved_entities,\n",
       "\u001B[1;32m   2746\u001B[0m                               served_models\u001B[38;5;241m=\u001B[39mserved_models,\n",
       "\u001B[0;32m-> 2747\u001B[0m                               traffic_config\u001B[38;5;241m=\u001B[39mtraffic_config)\u001B[38;5;241m.\u001B[39mresult(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85/lib/python3.11/site-packages/databricks/sdk/service/_internal.py:67\u001B[0m, in \u001B[0;36mWait.result\u001B[0;34m(self, timeout, callback)\u001B[0m\n",
       "\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mresult\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m     64\u001B[0m            timeout: datetime\u001B[38;5;241m.\u001B[39mtimedelta \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mtimedelta(minutes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m),\n",
       "\u001B[1;32m     65\u001B[0m            callback: Callable[[ReturnType], \u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ReturnType:\n",
       "\u001B[1;32m     66\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bind\u001B[38;5;241m.\u001B[39mcopy()\n",
       "\u001B[0;32m---> 67\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_waiter(callback\u001B[38;5;241m=\u001B[39mcallback, timeout\u001B[38;5;241m=\u001B[39mtimeout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85/lib/python3.11/site-packages/databricks/sdk/service/serving.py:2318\u001B[0m, in \u001B[0;36mServingEndpointsAPI.wait_get_serving_endpoint_not_updating\u001B[0;34m(self, name, timeout, callback)\u001B[0m\n",
       "\u001B[1;32m   2316\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(sleep \u001B[38;5;241m+\u001B[39m random\u001B[38;5;241m.\u001B[39mrandom())\n",
       "\u001B[1;32m   2317\u001B[0m     attempt \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[0;32m-> 2318\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimed out after \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtimeout\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstatus_message\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "\u001B[0;31mTimeoutError\u001B[0m: timed out after 0:20:00: current status: EndpointStateConfigUpdate.IN_PROGRESS"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TimeoutError",
        "evalue": "timed out after 0:20:00: current status: EndpointStateConfigUpdate.IN_PROGRESS"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>TimeoutError</span>: timed out after 0:20:00: current status: EndpointStateConfigUpdate.IN_PROGRESS"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTimeoutError\u001B[0m                              Traceback (most recent call last)",
        "File \u001B[0;32m<command-634720160579868>, line 38\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpdating the endpoint \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mserving_endpoint_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m to version \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlatest_model_version\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, this will take a few minutes to package and deploy the endpoint...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 38\u001B[0m     w\u001B[38;5;241m.\u001B[39mserving_endpoints\u001B[38;5;241m.\u001B[39mupdate_config_and_wait(served_models\u001B[38;5;241m=\u001B[39mendpoint_config\u001B[38;5;241m.\u001B[39mserved_models, name\u001B[38;5;241m=\u001B[39mserving_endpoint_name)\n\u001B[1;32m     40\u001B[0m displayHTML(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYour Model Endpoint Serving is now available. Open the <a href=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/ml/endpoints/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mserving_endpoint_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m>Model Serving Endpoint page</a> for more details.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85/lib/python3.11/site-packages/databricks/sdk/service/serving.py:2747\u001B[0m, in \u001B[0;36mServingEndpointsAPI.update_config_and_wait\u001B[0;34m(self, name, auto_capture_config, served_entities, served_models, traffic_config, timeout)\u001B[0m\n\u001B[1;32m   2734\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate_config_and_wait\u001B[39m(\n\u001B[1;32m   2735\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   2736\u001B[0m     name: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2741\u001B[0m     traffic_config: Optional[TrafficConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   2742\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimedelta(minutes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m)) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ServingEndpointDetailed:\n\u001B[1;32m   2743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_config(auto_capture_config\u001B[38;5;241m=\u001B[39mauto_capture_config,\n\u001B[1;32m   2744\u001B[0m                               name\u001B[38;5;241m=\u001B[39mname,\n\u001B[1;32m   2745\u001B[0m                               served_entities\u001B[38;5;241m=\u001B[39mserved_entities,\n\u001B[1;32m   2746\u001B[0m                               served_models\u001B[38;5;241m=\u001B[39mserved_models,\n\u001B[0;32m-> 2747\u001B[0m                               traffic_config\u001B[38;5;241m=\u001B[39mtraffic_config)\u001B[38;5;241m.\u001B[39mresult(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85/lib/python3.11/site-packages/databricks/sdk/service/_internal.py:67\u001B[0m, in \u001B[0;36mWait.result\u001B[0;34m(self, timeout, callback)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mresult\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     64\u001B[0m            timeout: datetime\u001B[38;5;241m.\u001B[39mtimedelta \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mtimedelta(minutes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m),\n\u001B[1;32m     65\u001B[0m            callback: Callable[[ReturnType], \u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ReturnType:\n\u001B[1;32m     66\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bind\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m---> 67\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_waiter(callback\u001B[38;5;241m=\u001B[39mcallback, timeout\u001B[38;5;241m=\u001B[39mtimeout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0cb78985-5009-4b81-a126-71e24b57eb85/lib/python3.11/site-packages/databricks/sdk/service/serving.py:2318\u001B[0m, in \u001B[0;36mServingEndpointsAPI.wait_get_serving_endpoint_not_updating\u001B[0;34m(self, name, timeout, callback)\u001B[0m\n\u001B[1;32m   2316\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(sleep \u001B[38;5;241m+\u001B[39m random\u001B[38;5;241m.\u001B[39mrandom())\n\u001B[1;32m   2317\u001B[0m     attempt \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 2318\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimed out after \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtimeout\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstatus_message\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
        "\u001B[0;31mTimeoutError\u001B[0m: timed out after 0:20:00: current status: EndpointStateConfigUpdate.IN_PROGRESS"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create or update serving endpoint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedModelInput, ServedModelInputWorkloadSize\n",
    "import requests\n",
    "\n",
    "serving_endpoint_name = \"pinecone_rag_chain\"\n",
    "latest_model_version = get_latest_model_version(model_name)\n",
    "\n",
    "databricks_api_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "\n",
    "w = WorkspaceClient()\n",
    "endpoint_config = EndpointCoreConfigInput(\n",
    "    name=serving_endpoint_name,\n",
    "    served_models=[\n",
    "        ServedModelInput(\n",
    "            model_name=model_name,\n",
    "            model_version=latest_model_version,\n",
    "            workload_size=ServedModelInputWorkloadSize.SMALL,\n",
    "            scale_to_zero_enabled=True,\n",
    "            environment_vars={\n",
    "                \"PINECONE_API_KEY\": \"{{secrets/prasad_kona/PINECONE_API_KEY}}\",\n",
    "                \"DATABRICKS_TOKEN\": \"{{secrets/dbdemos/rag_sp_token}}\",\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "existing_endpoint = next(\n",
    "    (e for e in w.serving_endpoints.list() if e.name == serving_endpoint_name), None\n",
    ")\n",
    "serving_endpoint_url = f\"{host}/ml/endpoints/{serving_endpoint_name}\"\n",
    "if existing_endpoint == None:\n",
    "    print(f\"Creating the endpoint {serving_endpoint_url}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.create_and_wait(name=serving_endpoint_name, config=endpoint_config)\n",
    "else:\n",
    "    print(f\"Updating the endpoint {serving_endpoint_url} to version {latest_model_version}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.update_config_and_wait(served_models=endpoint_config.served_models, name=serving_endpoint_name)\n",
    "    \n",
    "displayHTML(f'Your Model Endpoint Serving is now available. Open the <a href=\"/ml/endpoints/{serving_endpoint_name}\">Model Serving Endpoint page</a> for more details.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3414656-1ab8-4f34-afe1-08d261c311aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Our endpoint is now deployed! You can search endpoint name on the [Serving Endpoint UI](#/mlflow/endpoints) and visualize its performance!\n",
    "\n",
    "Let's run a REST query to try it in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7219e62c-af9d-473c-946f-a7e5ce925925",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest_model_version=3\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "w = WorkspaceClient()\n",
    "serving_endpoint_name = \"pinecone_rag_chain\"\n",
    "latest_model_version = get_latest_model_version(model_name)\n",
    "print(\"latest_model_version=\"+str(latest_model_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19580ada-21bd-418c-8240-f17f1649922a",
     "showTitle": true,
     "title": "Let's try to send a query to our chatbot"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 'Yes, Apache Spark supports streaming through Spark Structured Streaming, which is a scalable and fault-tolerant stream processing engine. It provides an easy-to-use API for creating continuous, real-time data pipelines.', 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf']}\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk.service.serving import DataframeSplitInput\n",
    "\n",
    "test_dialog = DataframeSplitInput(\n",
    "    columns=[\"messages\"],\n",
    "    data=[\n",
    "        \n",
    "            {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"What is Apache Spark?\"},\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": \"Apache Spark is an open-source data processing engine that is widely used in big data analytics.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": \"Does it support streaming?\"},\n",
    "                ]\n",
    "            }\n",
    "        \n",
    "    ],\n",
    ")\n",
    "answer = w.serving_endpoints.query(serving_endpoint_name, dataframe_split=test_dialog)\n",
    "print(answer.predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee721c2c-8d38-491e-8e27-c296fa015d1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 'Billing on Databricks is usage-based. Customers are charged according to the number of Databricks Units (DBUs) consumed. A DBU is a unit of measure for the processing power used in Databricks, which includes the use of compute resources and managed services. The cost per DBU depends on the type of Databricks Runtime and the cloud service provider. Databricks provides detailed usage reports to help customers monitor and manage their costs.', 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/technical_guide_solving_common-data-challenges-for-startups-and-digital-native-businesses.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/The-Data-Teams-Guide-to-the-DB-Lakehouse-Platform.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/databricks_ebook_insurance_v10.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/the-big-book-of-mlops-v10-072023.pdf']}\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk.service.serving import DataframeSplitInput\n",
    "\n",
    "test_dialog = DataframeSplitInput(\n",
    "    columns=[\"messages\"],\n",
    "    data=[\n",
    "        \n",
    "            {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"How does billing work on Databricks?\"},\n",
    "                    \n",
    "                ]\n",
    "            }\n",
    "        \n",
    "    ],\n",
    ")\n",
    "answer = w.serving_endpoints.query(serving_endpoint_name, dataframe_split=test_dialog)\n",
    "print(answer.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbfdcb71-979f-4e97-9abf-8b47d1c64ab9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Congratulations! You have deployed your RAG application with Pinecone!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2.2 - Advanced-Chatbot-Driver - Using Pinecone",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
