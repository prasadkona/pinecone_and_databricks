{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "778524dc-cde0-4eab-b79c-0fb733eb1c69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2/ Advanced chatbot with message history and filter using Langchain\n",
    "\n",
    "<img src=\"https://github.com/prasadkona/databricks_demos/blob/main/images/llm-rag-full-pinecone-0i.png?raw=true\" style=\"float: right; margin-left: 10px\"  width=\"900px;\">\n",
    "\n",
    "Data is now available on the Pinecone vector database!\n",
    "\n",
    "Let's now create a more advanced langchain model to perform RAG.\n",
    "\n",
    "We will improve our langchain model with the following:\n",
    "\n",
    "- Build a complete chain supporting a chat history, using llama 2 input style\n",
    "- Add a filter to only answer Databricks-related questions\n",
    "- Compute the embeddings with Databricks BGE models within our chain to query the Pinecone vector database\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1444828305810485&notebook=02-Deploy-RAG-Chatbot-Model&demo_name=chatbot-rag-llm&event=VIEW\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c8677f-bd35-4386-ab28-4714d9410a79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U mlflow==2.15.1 pinecone-client==5.0.1 langchain-pinecone==0.1.3 langchain==0.2.10\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f54d054-9785-40e4-8d8b-85440077c90a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# url used to send the request to your model from the serverless endpoint\n",
    "#host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "pinecone_index_name = \"dbdemo-index\"\n",
    "pinecone_namespace = 'dbdemo-namespace'\n",
    "\n",
    "catalog = \"prasad_kona_dev\"\n",
    "db = \"rag_chatbot_prasad_kona\"\n",
    "\n",
    "# Set a debug flag\n",
    "# Set the debug flag to True to test this notebook\n",
    "debug_flag = False\n",
    "\n",
    "if debug_flag:\n",
    "  ###### uncomment the next 3 lines to test this notebook\n",
    "  ###### comment the next 3 lines to run the driver notebook for deploy to UC and model serving\n",
    "  #pinecone_api_key = dbutils.secrets.get(\"pinecone_secrets_scope\", \"PINECONE_API_KEY\")\n",
    "  #os.environ[\"PINECONE_API_KEY\"] = dbutils.secrets.get(\"pinecone_secrets_scope\", \"PINECONE_API_KEY\")\n",
    "  #os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(\"pinecone_secrets_scope\", \"DATABRICKS_TOKEN\")\n",
    "  pinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "else:\n",
    "  pinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "  #pinecone_api_key = dbutils.secrets.get(\"pinecone_secrets_scope\", \"PINECONE_API_KEY\")\n",
    "  #os.environ[\"PINECONE_API_KEY\"] = dbutils.secrets.get(\"pinecone_secrets_scope\", \"PINECONE_API_KEY\")\n",
    "  #os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(\"pinecone_secrets_scope\", \"DATABRICKS_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68fdc723-f521-435c-a891-a41e1ca6af15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Try out Pinecone search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80fb084d-f97a-4d6b-a818-ca217011bbbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if debug_flag:\n",
    "  from pinecone import Pinecone\n",
    "  from langchain_pinecone import PineconeVectorStore\n",
    "  from langchain_community.embeddings import DatabricksEmbeddings\n",
    "  from pprint import pprint\n",
    "\n",
    "  embedding_model = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
    "\n",
    "\n",
    "  # connect to pinecone index\n",
    "  pc = Pinecone(api_key=pinecone_api_key)\n",
    "  index_pc = pc.Index(pinecone_index_name)\n",
    "  vectorstore = PineconeVectorStore(  \n",
    "      index=index_pc,\n",
    "      namespace=pinecone_namespace,\n",
    "      embedding=embedding_model, \n",
    "      text_key=\"content\"  \n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c161778-7017-4dad-8fbf-dc574a8b71ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(metadata={'id': 761.0, 'url': 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-and-ai-use-cases-for-the-public-sector.pdf'}, page_content='Founded by the original creators of Apache Spark,™ Delta Lake and MLflow, Databricks is on a \\nmission to help data teams solve the world’s toughest problems. To learn more, follow Databricks on Twitter , LinkedIn and Facebook.\\n© Databricks 2022. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation. Privacy Policy | Terms of Use')\n"
     ]
    }
   ],
   "source": [
    "if debug_flag:\n",
    "  query = \"What is Apache Spark?\"\n",
    "  docs = vectorstore.similarity_search(  \n",
    "      query,  # our search query  \n",
    "      k=3  # return 3 most relevant docs  \n",
    "  )  \n",
    "  pprint(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15ab6bb1-b884-444e-8ff1-9a46ae542ea2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exploring Langchain capabilities\n",
    "\n",
    "Let's start with the basics and send a query to a Databricks Foundation Model using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "295ff04a-e61c-419b-afec-7e11cff33a38",
     "showTitle": true,
     "title": "Spark Chat Model Prompt"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is an open-source, distributed computing system used for big data processing and analytics. It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Spark can run on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud, and is capable of accessing diverse data sources. It includes libraries for SQL and DataFrames, MLlib for machine learning, GraphX for graph processing, and Spark Streaming for real-time data processing.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  input_variables = [\"question\"],\n",
    "  template = \"You are an assistant. Give a short answer ot this question: {question}\"\n",
    ")\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-dbrx-instruct\", max_tokens = 500)\n",
    "\n",
    "chain = (\n",
    "  prompt\n",
    "  | chat_model\n",
    "  | StrOutputParser()\n",
    ")\n",
    "# test the chain\n",
    "if debug_flag:\n",
    "  print(chain.invoke({\"question\": \"What is Spark?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f058dc10-72b9-499c-b491-630b61957233",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Adding conversation history to the prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e492b25-1a87-482b-9e80-b22150efe1bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt_with_history_str = \"\"\"\n",
    "Your are a Big Data chatbot. Please answer Big Data question only. If you don't know or not related to Big Data, don't answer.\n",
    "\n",
    "Here is a history between you and a human: {chat_history}\n",
    "\n",
    "Now, please answer this question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_with_history = PromptTemplate(\n",
    "  input_variables = [\"chat_history\", \"question\"],\n",
    "  template = prompt_with_history_str\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d852a7-279a-484a-b353-b76b38095552",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When invoking our chain, we'll pass history as a list, specifying whether each message was sent by a user or the assistant. For example:\n",
    "\n",
    "```\n",
    "[\n",
    "  {\"role\": \"user\", \"content\": \"What is Apache Spark?\"}, \n",
    "  {\"role\": \"assistant\", \"content\": \"Apache Spark is an open-source data processing engine that is widely used in big data analytics.\"}, \n",
    "  {\"role\": \"user\", \"content\": \"Does it support streaming?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Let's create chain components to transform this input into the inputs passed to `prompt_with_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5bfe3ef-b62c-4565-8b5d-5de19420b7b1",
     "showTitle": true,
     "title": "Chat History Extractor Chain"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Apache Spark does support streaming. It has a built-in component called Spark Streaming that allows for the processing of real-time data streams. This makes it suitable for use cases such as real-time analytics, log processing, and sensor data processing.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "from operator import itemgetter\n",
    "\n",
    "#The question is the last entry of the history\n",
    "def extract_question(input):\n",
    "    return input[-1][\"content\"]\n",
    "\n",
    "#The history is everything before the last question\n",
    "def extract_history(input):\n",
    "    return input[:-1]\n",
    "\n",
    "chain_with_history = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_question),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_history),\n",
    "    }\n",
    "    | prompt_with_history\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "if debug_flag:\n",
    "    print(chain_with_history.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is Apache Spark?\"}, \n",
    "            {\"role\": \"assistant\", \"content\": \"Apache Spark is an open-source data processing engine that is widely used in big data analytics.\"}, \n",
    "            {\"role\": \"user\", \"content\": \"Does it support streaming?\"}\n",
    "        ]\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca284574-df83-493d-ad07-ea5de939216f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Let's add a filter on top to only answer Databricks-related questions.\n",
    "\n",
    "We want our chatbot to be profesionnal and only answer questions related to Databricks. Let's create a small chain and add a first classification step. \n",
    "\n",
    "*Note: this is a fairly naive implementation, another solution could be adding a small classification model based on the question embedding, providing faster classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e43dae0c-10a7-48a1-af64-438da692460d",
     "showTitle": true,
     "title": "Databricks Inquiry Classifier"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes.\n"
     ]
    }
   ],
   "source": [
    "chat_model = ChatDatabricks(endpoint=\"databricks-dbrx-instruct\", max_tokens = 200)\n",
    "\n",
    "is_question_about_databricks_str = \"\"\"\n",
    "You are classifying documents to know if this question is related with Databricks in AWS, Azure and GCP, Workspaces, Databricks account and cloud infrastructure setup, Data Science, Data Engineering, Big Data, Datawarehousing, SQL, Python and Scala or something from a very different field. Also answer no if the last part is inappropriate. \n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Question: Knowing this followup history: What is Databricks?, classify this question: Do you have more details?\n",
    "Expected Response: Yes\n",
    "\n",
    "Question: Knowing this followup history: What is Databricks?, classify this question: Write me a song.\n",
    "Expected Response: No\n",
    "\n",
    "Only answer with \"yes\" or \"no\". \n",
    "\n",
    "Knowing this followup history: {chat_history}, classify this question: {question}\n",
    "\"\"\"\n",
    "\n",
    "is_question_about_databricks_prompt = PromptTemplate(\n",
    "  input_variables= [\"chat_history\", \"question\"],\n",
    "  template = is_question_about_databricks_str\n",
    ")\n",
    "\n",
    "is_about_databricks_chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_question),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_history),\n",
    "    }\n",
    "    | is_question_about_databricks_prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "#test the chain\n",
    "if debug_flag:\n",
    "\n",
    "    #Returns \"Yes\" as this is about Databricks: \n",
    "    print(is_about_databricks_chain.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is Apache Spark?\"}, \n",
    "            {\"role\": \"assistant\", \"content\": \"Apache Spark is an open-source data processing engine that is widely used in big data analytics.\"}, \n",
    "            {\"role\": \"user\", \"content\": \"Does it support streaming?\"}\n",
    "        ]\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df29086d-480d-4b9a-a832-6a54e5ee0c46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.\n"
     ]
    }
   ],
   "source": [
    "if debug_flag:\n",
    "    #Return \"no\" as this isn't about Databricks\n",
    "    print(is_about_databricks_chain.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is the meaning of life?\"}\n",
    "        ]\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ce3d14d-4b67-4070-befe-980c2f25e20a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Use LangChain to retrieve documents from the Pinecone vector database\n",
    "\n",
    "\n",
    "Let's add our Langchain retriever. \n",
    "\n",
    "It will be in charge of:\n",
    "\n",
    "* Creating the input question embeddings (with Databricks `bge-large-en`)\n",
    "* Calling the Pinecone vector database to find similar documents to augment the prompt with\n",
    "\n",
    "Langchain wrapper makes it easy to do in one step, handling all the underlying logic and API call for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dbfc55d-f879-4adb-9bd8-69f3163837c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "set_debug(False)\n",
    "\n",
    "#host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea24d51-eaa3-49de-a357-55d29b4ce498",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'id': 761.0, 'url': 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-and-ai-use-cases-for-the-public-sector.pdf'}, page_content='Founded by the original creators of Apache Spark,™ Delta Lake and MLflow, Databricks is on a \\nmission to help data teams solve the world’s toughest problems. To learn more, follow Databricks on Twitter , LinkedIn and Facebook.\\n© Databricks 2022. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation. Privacy Policy | Terms of Use'), Document(metadata={'id': 220.0, 'url': 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/databricks-eBook-finServ-cyber.pdf'}, page_content='All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation. Privacy Policy | Terms of Use'), Document(metadata={'id': 120.0, 'url': 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/building-reliable-data-lakes-at-scale-with-delta-lake.pdf'}, page_content='What matters  \\nmore is the ability to have robust, production data pipelines at scale without \\nrequiring  high operational overhead.Key Goals\\n4\\nApache Spark ™: The First Unified Analytics Engine\\nOriginally developed at UC Berkeley in 2009, Apache Spark can be \\nconsidered the first unified analytics engine. Uniquely bringing data \\nand AI technologies together, Spark comes packaged with higher-level \\nlibraries, including support for SQL queries, streaming data, machine \\nlearning and graph processing. These standard libraries increase \\ndeveloper productivity and can be seamlessly combined to create \\ncomplex workflows.\\nSince its release, Apache Spark, has seen rapid adoption by \\nenterprises across a wide range of industries. Internet powerhouses \\nsuch as Netflix, Yahoo and eBay have deployed Spark at massive scale, \\ncollectively processing multiple petabytes of data on clusters of over \\n8,000 nodes making it the de facto choice for new analytics initiatives. \\nIt has quickly become the largest open source community in big data, \\nwith over 1000 contributors from 250+ organizations.Apache Spark™\\nClick \\nStreams\\n...Video/ \\nSpeec h\\nSensor\\nData (IoT)Emails/\\nWeb PagesCustomer \\nData\\nBig Dat a Pr ocessing\\nETL + SQL + Str eaming MLlib + Sp arkRMachine L earning\\nWhile Spark has had a significant impact in taking data analytics \\nto the next level, practitioners continue to face data reliability and \\nperformance challenges with their data lakes.\\n55\\nData Reliability Challenges With Data Lakes\\nSchema Mismatch — When ingesting content from multiple sources, typical of \\nlarge, modern big data environments, it can be difficult to ensure that the same \\ndata is encoded in the same way i.e., the schema matches. A similar challenge \\narises when the formats for  data elements are changed without informing the \\ndata engineering team.'), Document(metadata={'id': 1366.0, 'url': 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/Why-the-Data-Lakehouse-Is-Your-Next-Data-Warehouse-Ebook-2nd%20Edition.pdf'}, page_content='Databricks is headquartered in San Francisco, \\nwith offices around the globe. Founded by the original creators of \\nApache Spark,TM Delta Lake and MLflow, Databricks is on a mission to help \\ndata teams solve the world’s toughest problems. To learn more, follow \\nDatabricks on Twitter , LinkedIn and Facebook.About Databricks\\nContact us for a personalized demo  \\ndatabricks.com/contactSTART YOUR FREE TRIAL\\nDISCOVER LAKEHOUSE\\n© Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation. Privacy Policy | Terms of Use')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import DatabricksEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "from langchain.globals import set_verbose\n",
    "\n",
    "set_debug(False)\n",
    "set_verbose(False)\n",
    "\n",
    "\n",
    "embedding_model = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
    "\n",
    "def get_retriever(persist_dir: str = None):\n",
    "    #os.environ[\"DATABRICKS_HOST\"] = host\n",
    "    # initialize pinecone and connect to pinecone index\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    index_pc = pc.Index(pinecone_index_name)\n",
    "\n",
    "    vectorstore = PineconeVectorStore(  \n",
    "        index=index_pc,\n",
    "        namespace=pinecone_namespace,\n",
    "        embedding=embedding_model, \n",
    "        text_key=\"content\"  \n",
    "    )\n",
    "\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "retriever = get_retriever()\n",
    "\n",
    "retrieve_document_chain = (\n",
    "    itemgetter(\"messages\") \n",
    "    | RunnableLambda(extract_question)\n",
    "    | retriever\n",
    ")\n",
    "\n",
    "#test the retriever chain\n",
    "if debug_flag:\n",
    "    print(retrieve_document_chain.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is Apache Spark?\"}]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7052b72f-5924-47b6-8600-93c987e99e03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Improve document search using LLM to generate a better sentence for the vector store, based on the chat history\n",
    "\n",
    "We need to retrieve documents related the the last question but also the history.\n",
    "\n",
    "One solution is to add one step to add our LLM to summarize the history and the last question, making it a better fit for our vector search query. Let's do that as a new step in our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5d1c888-132c-4cd7-9e9d-a27be2a58ac9",
     "showTitle": true,
     "title": "Contextual Query Generation Chain"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test retriever query without history: What is Apache Spark?\nTest retriever question, summarized with history: \"Apache Spark and its streaming capabilities for real-time data processing\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "generate_query_to_retrieve_context_template = \"\"\"\n",
    "Based on the chat history below, we want you to generate a query for an external data source to retrieve relevant documents so that we can better answer the question. The query should be in natual language. The external data source uses similarity search to search for relevant documents in a vector space. So the query should be similar to the relevant documents semantically. Answer with only the query. Do not add explanation.\n",
    "\n",
    "Chat history: {chat_history}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "generate_query_to_retrieve_context_prompt = PromptTemplate(\n",
    "  input_variables= [\"chat_history\", \"question\"],\n",
    "  template = generate_query_to_retrieve_context_template\n",
    ")\n",
    "\n",
    "generate_query_to_retrieve_context_chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_question),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_history),\n",
    "    }\n",
    "    | RunnableBranch(  #Augment query only when there is a chat history\n",
    "      (lambda x: x[\"chat_history\"], generate_query_to_retrieve_context_prompt | chat_model | StrOutputParser()),\n",
    "      (lambda x: not x[\"chat_history\"], RunnableLambda(lambda x: x[\"question\"])),\n",
    "      RunnableLambda(lambda x: x[\"question\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "if debug_flag:\n",
    "    #Let's try it\n",
    "    output = generate_query_to_retrieve_context_chain.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is Apache Spark?\"}\n",
    "        ]\n",
    "    })\n",
    "    print(f\"Test retriever query without history: {output}\")\n",
    "\n",
    "    output = generate_query_to_retrieve_context_chain.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is Apache Spark?\"}, \n",
    "            {\"role\": \"assistant\", \"content\": \"Apache Spark is an open-source data processing engine that is widely used in big data analytics.\"}, \n",
    "            {\"role\": \"user\", \"content\": \"Does it support streaming?\"}\n",
    "        ]\n",
    "    })\n",
    "    print(f\"Test retriever question, summarized with history: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a78aa6-9d99-4d81-ac34-698f3856a7aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Let's put it together\n",
    "\n",
    "<img src=\"https://github.com/prasadkona/databricks_demos/blob/main/images/llm-rag-full-pinecone-7.png?raw=true\" style=\"float: right\" width=\"600px\">\n",
    "\n",
    "\n",
    "Let's now merge the retriever and the full Langchain chain.\n",
    "\n",
    "We will use a custom langchain template for our assistant to give proper answer.\n",
    "\n",
    "Make sure you take some time to try different templates and adjust your assistant tone and personality for your requirement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47eb1e55-fa7e-4227-afcb-86e27b07f58b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "question_with_history_and_context_str = \"\"\"\n",
    "You are a trustful assistant for Databricks users. You are answering python, coding, SQL, data engineering, spark, data science, AI, ML, Datawarehouse, platform, API or infrastructure, Cloud administration question related to Databricks. If you do not know the answer to a question, you truthfully say you do not know. Read the discussion to get the context of the previous conversation. In the chat discussion, you are referred to as \"system\". The user is referred to as \"user\".\n",
    "\n",
    "Discussion: {chat_history}\n",
    "\n",
    "Here's some context which might or might not help you answer: {context}\n",
    "\n",
    "Answer straight, do not repeat the question, do not start with something like: the answer to the question, do not add \"AI\" in front of your answer, do not say: here is the answer, do not mention the context or the question.\n",
    "\n",
    "Based on this history and context, answer this question: {question}\n",
    "\"\"\"\n",
    "\n",
    "question_with_history_and_context_prompt = PromptTemplate(\n",
    "  input_variables= [\"chat_history\", \"context\", \"question\"],\n",
    "  template = question_with_history_and_context_str\n",
    ")\n",
    "\n",
    "def format_context(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "def extract_source_urls(docs):\n",
    "    return [d.metadata[\"url\"] for d in docs]\n",
    "\n",
    "relevant_question_chain = (\n",
    "  RunnablePassthrough() |\n",
    "  {\n",
    "    \"relevant_docs\": generate_query_to_retrieve_context_prompt | chat_model | StrOutputParser() | retriever,\n",
    "    \"chat_history\": itemgetter(\"chat_history\"), \n",
    "    \"question\": itemgetter(\"question\")\n",
    "  }\n",
    "  |\n",
    "  {\n",
    "    \"context\": itemgetter(\"relevant_docs\") | RunnableLambda(format_context),\n",
    "    \"sources\": itemgetter(\"relevant_docs\") | RunnableLambda(extract_source_urls),\n",
    "    \"chat_history\": itemgetter(\"chat_history\"), \n",
    "    \"question\": itemgetter(\"question\")\n",
    "  }\n",
    "  |\n",
    "  {\n",
    "    \"prompt\": question_with_history_and_context_prompt,\n",
    "    \"sources\": itemgetter(\"sources\")\n",
    "  }\n",
    "  |\n",
    "  {\n",
    "    \"result\": itemgetter(\"prompt\") | chat_model | StrOutputParser(),\n",
    "    \"sources\": itemgetter(\"sources\")\n",
    "  }\n",
    ")\n",
    "\n",
    "irrelevant_question_chain = (\n",
    "  RunnableLambda(lambda x: {\"result\": 'I cannot answer questions that are not about Databricks.', \"sources\": []})\n",
    ")\n",
    "\n",
    "branch_node = RunnableBranch(\n",
    "  (lambda x: \"yes\" in x[\"question_is_relevant\"].lower(), relevant_question_chain),\n",
    "  (lambda x: \"no\" in x[\"question_is_relevant\"].lower(), irrelevant_question_chain),\n",
    "  irrelevant_question_chain\n",
    ")\n",
    "\n",
    "full_chain = (\n",
    "  {\n",
    "    \"question_is_relevant\": is_about_databricks_chain,\n",
    "    \"question\": itemgetter(\"messages\") | RunnableLambda(extract_question),\n",
    "    \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_history),    \n",
    "  }\n",
    "  | branch_node\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c467b3-ed6a-4370-89bd-c30d8955a2e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's try our full chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d326989-6a81-43af-9680-869ba64cab98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if debug_flag:\n",
    "  def display_chat(chat_history, response):\n",
    "    def user_message_html(message):\n",
    "      return f\"\"\"\n",
    "        <div style=\"width: 90%; border-radius: 10px; background-color: #c2efff; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; font-size: 14px;\">\n",
    "          {message}\n",
    "        </div>\"\"\"\n",
    "    def assistant_message_html(message):\n",
    "      return f\"\"\"\n",
    "        <div style=\"width: 90%; border-radius: 10px; background-color: #e3f6fc; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; margin-left: 40px; font-size: 14px\">\n",
    "          <img style=\"float: left; width:40px; margin: -10px 5px 0px -10px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/robot.png?raw=true\"/>\n",
    "          {message}\n",
    "        </div>\"\"\"\n",
    "    chat_history_html = \"\".join([user_message_html(m[\"content\"]) if m[\"role\"] == \"user\" else assistant_message_html(m[\"content\"]) for m in chat_history])\n",
    "    answer = response[\"result\"].replace('\\n', '<br/>')\n",
    "    sources_html = (\"<br/><br/><br/><strong>Sources:</strong><br/> <ul>\" + '\\n'.join([f\"\"\"<li><a href=\"{s}\">{s}</a></li>\"\"\" for s in response[\"sources\"]]) + \"</ul>\") if response[\"sources\"] else \"\"\n",
    "    response_html = f\"\"\"{answer}{sources_html}\"\"\"\n",
    "\n",
    "    displayHTML(chat_history_html + assistant_message_html(response_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "034667da-1d19-42ea-b4e1-19ebcf0bf474",
     "showTitle": true,
     "title": "Asking an out-of-scope question"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with a non relevant question...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 90%; border-radius: 10px; background-color: #c2efff; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; font-size: 14px;\">\n",
       "          What is Apache Spark?\n",
       "        </div>\n",
       "        <div style=\"width: 90%; border-radius: 10px; background-color: #e3f6fc; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; margin-left: 40px; font-size: 14px\">\n",
       "          <img style=\"float: left; width:40px; margin: -10px 5px 0px -10px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/robot.png?raw=true\"/>\n",
       "          Apache Spark is an open-source data processing engine that is widely used in big data analytics.\n",
       "        </div>\n",
       "        <div style=\"width: 90%; border-radius: 10px; background-color: #c2efff; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; font-size: 14px;\">\n",
       "          Why is the sky blue?\n",
       "        </div>\n",
       "        <div style=\"width: 90%; border-radius: 10px; background-color: #e3f6fc; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; margin-left: 40px; font-size: 14px\">\n",
       "          <img style=\"float: left; width:40px; margin: -10px 5px 0px -10px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/robot.png?raw=true\"/>\n",
       "          I cannot answer questions that are not about Databricks.\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "if debug_flag:\n",
    "    non_relevant_dialog = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is Apache Spark?\"}, \n",
    "            {\"role\": \"assistant\", \"content\": \"Apache Spark is an open-source data processing engine that is widely used in big data analytics.\"}, \n",
    "            {\"role\": \"user\", \"content\": \"Why is the sky blue?\"}\n",
    "        ]\n",
    "    }\n",
    "    print(f'Testing with a non relevant question...')\n",
    "    response = full_chain.invoke(non_relevant_dialog)\n",
    "    display_chat(non_relevant_dialog[\"messages\"], response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0d7aeb-49e8-444d-9731-1fda23c38586",
     "showTitle": true,
     "title": "Asking a relevant question"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with relevant history and question...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"width: 90%; border-radius: 10px; background-color: #c2efff; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; font-size: 14px;\">\n",
       "          What is Apache Spark?\n",
       "        </div>\n",
       "        <div style=\"width: 90%; border-radius: 10px; background-color: #e3f6fc; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; margin-left: 40px; font-size: 14px\">\n",
       "          <img style=\"float: left; width:40px; margin: -10px 5px 0px -10px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/robot.png?raw=true\"/>\n",
       "          Apache Spark is an open-source data processing engine that is widely used in big data analytics.\n",
       "        </div>\n",
       "        <div style=\"width: 90%; border-radius: 10px; background-color: #c2efff; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; font-size: 14px;\">\n",
       "          Does it support streaming?\n",
       "        </div>\n",
       "        <div style=\"width: 90%; border-radius: 10px; background-color: #e3f6fc; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; margin-left: 40px; font-size: 14px\">\n",
       "          <img style=\"float: left; width:40px; margin: -10px 5px 0px -10px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/robot.png?raw=true\"/>\n",
       "          Yes.\n",
       "        </div>\n",
       "        <div style=\"width: 90%; border-radius: 10px; background-color: #c2efff; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; font-size: 14px;\">\n",
       "          Tell me more about it's capabilities.\n",
       "        </div>\n",
       "        <div style=\"width: 90%; border-radius: 10px; background-color: #e3f6fc; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; margin-left: 40px; font-size: 14px\">\n",
       "          <img style=\"float: left; width:40px; margin: -10px 5px 0px -10px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/robot.png?raw=true\"/>\n",
       "          Apache Spark is a powerful data processing engine that supports streaming workloads, enabling low-latency streaming data pipelines. It can directly ingest data from event buses like Apache Kafka, AWS Kinesis, Confluent Cloud, Amazon MSK, or Azure Event Hubs. Spark Structured Streaming UI provides insights into potential latency issues and failure scenarios. It also offers monitoring and alerting capabilities for production streaming workloads, allowing you to observe resource usage effectively. Additionally, Spark supports geospatial data analysis and provides tools like the UDF profiler for optimizing user-defined functions. Delta Live Tables (DLT) is an ETL framework that uses a simple declarative approach for creating reliable data pipelines and fully manages the underlying infrastructure at scale for batch and streaming data.<br/><br/><br/><strong>Sources:</strong><br/> <ul><li><a href=\"dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf\">dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf</a></li>\n",
       "<li><a href=\"dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf\">dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf</a></li>\n",
       "<li><a href=\"dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf\">dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf</a></li>\n",
       "<li><a href=\"dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/030521-2-The-Delta-Lake-Series-Complete-Collection.pdf\">dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/030521-2-The-Delta-Lake-Series-Complete-Collection.pdf</a></li></ul>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if debug_flag:\n",
    "    dialog = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is Apache Spark?\"}, \n",
    "            {\"role\": \"assistant\", \"content\": \"Apache Spark is an open-source data processing engine that is widely used in big data analytics.\"}, \n",
    "            {\"role\": \"user\", \"content\": \"Does it support streaming?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Yes.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Tell me more about it's capabilities.\"},\n",
    "        ]\n",
    "    }\n",
    "    print(f'Testing with relevant history and question...')\n",
    "    response = full_chain.invoke(dialog)\n",
    "    display_chat(dialog[\"messages\"], response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de9bd71-b02e-43f3-ac0d-49682239260f",
     "showTitle": true,
     "title": "Setting the full chain for logging with mlfow"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.models.set_model(model=full_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bd5ae1b-a29a-4b23-8519-33d83de929db",
     "showTitle": true,
     "title": "Testing the chain with a input that includes history"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.......\n{'result': 'Apache Spark is a powerful data processing engine that supports streaming workloads, enabling low-latency streaming data pipelines. It can directly ingest data from event buses like Apache Kafka, AWS Kinesis, Confluent Cloud, Amazon MSK, or Azure Event Hubs. Spark Structured Streaming UI provides insights into potential latency issues and failure scenarios. It also offers monitoring and alerting capabilities for production streaming workloads, allowing you to observe resource usage effectively. Additionally, Spark supports geospatial data analysis and provides tools like the UDF profiler for optimizing user-defined functions. Delta Live Tables (DLT) is an ETL framework that uses a simple declarative approach for creating reliable data pipelines and fully manages the underlying infrastructure at scale for batch and streaming data.', 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/big-book-of-data-engineering-2nd-edition-final.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/030521-2-The-Delta-Lake-Series-Complete-Collection.pdf']}\nsignature.......\ninputs: \n  ['messages': Array({content: string (required), role: string (required)}) (required)]\noutputs: \n  ['result': string (required), 'sources': Array(string) (required)]\nparams: \n  None\n\n"
     ]
    }
   ],
   "source": [
    "if debug_flag:\n",
    "  #Get our model signature from input/output\n",
    "  from mlflow.models import infer_signature\n",
    "  output = full_chain.invoke(dialog)\n",
    "  signature = infer_signature(dialog, output)\n",
    "  print(\"output.......\")\n",
    "  print( output)\n",
    "  print(\"signature.......\")\n",
    "  print(signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a501da45-fbc0-48e8-ac1f-b1829082ad99",
     "showTitle": true,
     "title": "Testing the chain with a input that doesnot include history"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.......\n{'result': 'Billing on Databricks is based on usage and is typically charged to the cloud service provider account where the Databricks workspace is hosted. The billing is usually usage-based, meaning you only pay for the resources you use. This can lead to lower total cost of ownership compared to legacy Hadoop systems and can help reduce premiums for customers and lower loss ratios for insurance carriers. The serverless data plane network infrastructure is managed by Databricks in a Databricks cloud service provider account and shared among customers, with additional network boundaries between workspaces and between clusters.', 'sources': ['dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/technical_guide_solving_common-data-challenges-for-startups-and-digital-native-businesses.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/The-Data-Teams-Guide-to-the-DB-Lakehouse-Platform.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/databricks_ebook_insurance_v10.pdf', 'dbfs:/Volumes/prasad_kona_dev/rag_chatbot_prasad_kona/volume_databricks_documentation/databricks-pdf/the-big-book-of-mlops-v10-072023.pdf']}\n"
     ]
    }
   ],
   "source": [
    "if debug_flag:\n",
    "  #Get our model signature from input/output\n",
    "  from mlflow.models import infer_signature\n",
    "  input_example = {\n",
    "   \"messages\": [\n",
    "       {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": \"How does billing work on Databricks?\",\n",
    "       }\n",
    "   ]\n",
    "  }\n",
    "  output = full_chain.invoke(input_example)\n",
    "  print(\"output.......\")\n",
    "  print( output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbfdcb71-979f-4e97-9abf-8b47d1c64ab9",
     "showTitle": true,
     "title": ""
    }
   },
   "source": [
    "## Next we register this Rag chatbot chain with Databricks unity catalog and then deploy to model serving"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2.1 - Advanced-Chatbot-Chain - Using Pinecone",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
